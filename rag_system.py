import time
import torch
import random
import logging
import hashlib
import numpy as np
from tqdm import tqdm
from pathlib import Path
from typing import List, Optional, Dict

from llama_index.core import (
    VectorStoreIndex,  # åˆ›å»ºå‘é‡ç´¢å¼•å¯¹è±¡
    SimpleDirectoryReader,
    Document,
    StorageContext,  # ç´¢å¼•æ•°æ®è¦å­˜åœ¨å“ªé‡Œ æ€Žä¹ˆå­˜ ç”¨ä»€ä¹ˆæ–¹å¼å­˜
    load_index_from_storage,  # ç”¨äºŽä»Žæœ¬åœ°åŠ è½½å·²æœ‰çš„ç´¢å¼• è¦ä¼ å…¥ä¸€ä¸ªStorageContextå¯¹è±¡
    Settings
)

from llama_index.core.node_parser import SentenceSplitter  # node_parserç”¨åˆ°äº†
from llama_index.llms.openai_like import OpenAILike
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.memory import ChatMemoryBuffer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGConfig:
    # Document processing configuration
    CHUNK_SIZE = 512
    # CHUNK_SIZE = 1024
    # CHUNK_SIZE = 2048
    # CHUNK_OVERLAP = 200
    CHUNK_OVERLAP = 200
    SUPPORTED_EXTENSIONS = [".pdf", ".docx", ".doc", ".txt", ".md", ".html"]

    # LLM configuration
    DEFAULT_TEMPERATURE = 0.3
    # DEFAULT_MAX_TOKENS = 2048  # é™åˆ¶LLMç”Ÿæˆçš„æœ€å¤§tokenæ•°
    DEFAULT_MAX_TOKENS = 1024  # é™åˆ¶LLMç”Ÿæˆçš„æœ€å¤§tokenæ•°
    # DEFAULT_MAX_TOKENS = 512  # é™åˆ¶LLMç”Ÿæˆçš„æœ€å¤§tokenæ•°
    # DEFAULT_MAX_TOKENS = 768  # é™åˆ¶LLMç”Ÿæˆçš„æœ€å¤§tokenæ•°
    
    # Retrieval configuration
    SIMILARITY_TOP_K = 3  # æ ¹æ®å‘é‡æ‰¾ chunk      å‘é‡å’Œchunkçš„å‘é‡åšç›¸ä¼¼åº¦æ¯”è¾ƒ
    TOKEN_LIMIT = 4000  # å¤šè½®å¯¹è¯æ—¶ ä¿å­˜å¤šå°‘tokençš„åŽ†å²
    
    # Random seed
    RANDOM_SEED = 42

    
class SystemPrompts:
    DEFAULT = (
        "You are an intelligent conversational assistant for additive manufacturing based on Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs)."
        "Answer user questions strictly based on the retrieved document content and cite the specific document source in your answers."
        "If there is no relevant information in the retrieved document content, explicitly state: I do not know."
        "Do not generate answers that are not supported by the retrieved document content."
        "If asking the user a clarifying question would be helpful, please do so."
    )


    @staticmethod
    def folder_specific(folder_name: str) -> str:
        return (  
            "You are an intelligent conversational assistant for additive manufacturing based on Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs)."  
            f"You specialize in answering user questions strictly based on the content of documents retrieved from the '{folder_name}' folder."
            "In your answers, always cite the specific document source."
            "If there is no relevant information in the retrieved document content, explicitly state: I do not know. In this case, do not cite any document source."
            "Do not generate any answers that are not supported by the retrieved document content."
            "If asking the user a clarifying question would be helpful, please do so."
        )
    

class MultiModelRAGSystem:
    def __init__(self, api_key: str, model_type: str = "deepseek-chat", documents_dir: str = "./documents", storage_dir: str = "./storage", embedding_model: str = "BAAI/bge-large-en-v1.5", random_seed: Optional[int] = None):
        # Set random seed for reproducibility
        self.random_seed = random_seed or RAGConfig.RANDOM_SEED  # å¦‚æžœinitæ²¡ä¼  æœ‰é»˜è®¤çš„
        self._set_random_seeds()  # defined function below
        
        self.api_key = api_key
        self.model_type = model_type.lower()
        self.documents_dir = Path(documents_dir)
        self.storage_dir = Path(storage_dir)
        
        # Create directories
        self.documents_dir.mkdir(exist_ok=True)
        self.storage_dir.mkdir(exist_ok=True)
        
        # Configure LLM based on model type
        self.llm = self._configure_llm()  # åˆå§‹åŒ–LLMå¯¹è±¡ # defined function below
        
        logger.info(f"Using the LLM API: {self.model_type}.")
        
        # Configure embedding model (using local model with CUDA acceleration)
        device = "mps" if torch.backends.mps.is_available() else "cpu"
        # device = "cuda" if torch.cuda.is_available() else "cpu"
        
        logger.info(f"Detected Device in Current Environment: {device}.")

        self.embed_model = HuggingFaceEmbedding(
            model_name=embedding_model,
            cache_folder="./embedding_cache",
            device=device
        )
        
        # Configure global settings
        Settings.llm = self.llm
        Settings.embed_model = self.embed_model
        Settings.chunk_size = RAGConfig.CHUNK_SIZE
        Settings.chunk_overlap = RAGConfig.CHUNK_OVERLAP
        
        # Configure node parser
        self.node_parser = SentenceSplitter(
            chunk_size=RAGConfig.CHUNK_SIZE,
            chunk_overlap=RAGConfig.CHUNK_OVERLAP,
        )
        
        self.index = None
        self.chat_engine = None
        
        # Folder-based indexing functionality
        self.folder_indexes: Dict[str, VectorStoreIndex] = {}
        self.folder_chat_engines: Dict[str, any] = {}
    
    def _set_random_seeds(self) -> None:
        """Set random seeds for reproducibility"""
        random.seed(self.random_seed)
        np.random.seed(self.random_seed)
        torch.manual_seed(self.random_seed)  # PyTorchçš„CPUéšæœºæ•°ç”Ÿæˆå™¨
        if torch.cuda.is_available():  # GPU
            torch.cuda.manual_seed(self.random_seed)
            torch.cuda.manual_seed_all(self.random_seed)
        # Set deterministic behavior
        # è¿™ä¸¤è¡Œç”¨äºŽå½»åº•ä¿è¯æ“ä½œçš„ç¡®å®šæ€§ æœ‰çš„åº•å±‚å®žçŽ°æœ‰â€œéžç¡®å®šæ€§åŠ é€Ÿä¼˜åŒ–â€ è¿™æ ·å†™èƒ½å…³æŽ‰æ‰€æœ‰ä¸ç¡®å®šæ€§æ“ä½œ ç‰ºç‰²ä¸€ç‚¹é€Ÿåº¦æ¢æ¥â€œå®Œå…¨å¯å¤çŽ°â€
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        logger.info(f"Manually Set Random Seed: {self.random_seed} in All Libraries.")
    
    def _get_base_llm_config(self) -> dict:
        """Get base LLM configuration"""
        # ä¸“é—¨è´Ÿè´£åŸºç¡€å…¬å…±é…ç½® api_key temperature max_tokens è¿™äº›å‚æ•°éƒ½é€šç”¨
        return {
            'api_key': self.api_key,
            'temperature': RAGConfig.DEFAULT_TEMPERATURE,
            'max_tokens': RAGConfig.DEFAULT_MAX_TOKENS,
            'is_chat_model': True,  # å£°æ˜Žè¿™æ˜¯â€œå¯¹è¯å¤§æ¨¡åž‹â€
        }
    
    def _configure_llm(self):  # æ ¹æ®é€‰çš„æ¨¡åž‹ç±»åž‹ åŠ¨æ€åˆæˆå…¨é‡é…ç½® å¹¶åˆ›å»ºæ¨¡åž‹å¯¹è±¡
        """
        Configure LLM based on model type
        
        Returns:
            Configured LLM instance
        """
        base_config = self._get_base_llm_config()
        
        model_configs = {
            "deepseek-chat": {
                "model": "deepseek-chat",  # DeepSeek-V3-0324 Jul-22
                "api_base": "https://api.deepseek.com/v1",  
            },
            "deepseek-reasoner": {
                "model": "deepseek-reasoner",  # DeepSeek-R1-0528 JUl-22
                "api_base": "https://api.deepseek.com/v1",
            },
            "gpt4.1": {
                "model": "gpt-4.1-2025-04-14",
                "api_base": "https://api.openai.com/v1",
            },
            "gpt4o": {
                "model": "gpt-4o-2024-08-06",
                "api_base": "https://api.openai.com/v1",
            }
        }
        
        if self.model_type not in model_configs:
            raise ValueError(f"Unsupported LLM API: {self.model_type}. Supported Ones: {', '.join(model_configs.keys())}.")
        # base_config æ˜¯é€šç”¨çš„LLMé…ç½®ä¿¡æ¯;
        # model_configs æ˜¯å…·ä½“LLMå“ç‰Œçš„é…ç½®ä¿¡æ¯
        config = {**base_config, **model_configs[self.model_type]}
        return OpenAILike(**config)  # å‚æ•°è§£åŒ… Keyword Arguments unpacking
        
    def _add_document_metadata(self, doc: Document, folder_name: Optional[str] = None) -> None:
        """Add standard metadata to document"""
        # hasattr æ˜¯å¦æœ‰æŸä¸ªå±žæ€§
        if hasattr(doc, 'metadata') and 'file_path' in doc.metadata:
            file_path = Path(doc.metadata['file_path'])
            doc.metadata.update({
                'filename': file_path.name,
                'file_type': file_path.suffix,
                'file_size': file_path.stat().st_size if file_path.exists() else 0
            })
            if folder_name:
                doc.metadata['folder'] = folder_name
                
    def load_documents(self, file_extensions: Optional[List[str]] = None) -> List[Document]:
        """
        Load documents from specified directory
        
        Args:
            file_extensions: List of supported file extensions
            
        Returns:
            List of documents
        """
        if file_extensions is None:
            file_extensions = RAGConfig.SUPPORTED_EXTENSIONS
        
        try:
            reader = SimpleDirectoryReader(
                input_dir=str(self.documents_dir),
                recursive=True,
                required_exts=file_extensions,
            )
            
            documents = reader.load_data(show_progress=True,num_workers=4)
            logger.info(f"Successfully Loaded {len(documents)} Document Pages.")  # å¤šå°‘page
            
            # Add metadata to each document
            for doc in documents:
                self._add_document_metadata(doc)  # ç»™æ¯ä¸€ä¸ªdocæ·»åŠ å…ƒæ•°æ®
            ########################################################################################
            unique_docs = []
            seen_hashes = set()
            for doc in documents:
                content_hash = hashlib.md5(doc.text.encode('utf-8')).hexdigest()  # å¯¹æ¯ä¸ªé¡µé¢å†…å®¹ï¼ˆdoc.textï¼‰åšmd5å“ˆå¸Œè¿ç®—ï¼Œå¾—åˆ°ä¸€ä¸ªå”¯ä¸€çš„â€œæŒ‡çº¹â€ã€‚
                if content_hash not in seen_hashes:
                    seen_hashes.add(content_hash)
                    unique_docs.append(doc)
            logger.info(f"There Were {len(documents)} Document Pages Before.")
            logger.info(f"After Hash Deduplication, {len(unique_docs)} Document Pages Remain.")
            return unique_docs
            #########################################################################################
            # return documents
            
        except Exception as e:
            logger.error(f"Error on Loading Document Page: {e}.")
            return []
    
    def build_index(self, documents: List[Document], force_rebuild: bool = False) -> None:
        """
        Build or load vector index
        
        Args:
            documents: List of documents
            force_rebuild: Whether to force rebuild index
        """
        index_store_file = self.storage_dir / "index_store.json"
        docstore_file = self.storage_dir / "docstore.json"
        
        try:
            # Check if saved index exists
            if index_store_file.exists() and docstore_file.exists() and not force_rebuild:
                start_time = time.time()
                logger.info("Loading Existing Index for Global Chat Engine...")

                # StorageContext,  # ç´¢å¼•æ•°æ®è¦å­˜åœ¨å“ªé‡Œ æ€Žä¹ˆå­˜ ç”¨ä»€ä¹ˆæ–¹å¼å­˜
                # load_index_from_storage,  # ç”¨äºŽä»Žæœ¬åœ°åŠ è½½å·²æœ‰çš„ç´¢å¼• è¦ä¼ å…¥ä¸€ä¸ªStorageContextå¯¹è±¡

                with tqdm(total=1, desc="Loading Existing Index for Global Chat Engine.", unit="Step", leave=False, disable=True) as pbar:
                    storage_context = StorageContext.from_defaults(persist_dir=str(self.storage_dir))  # åˆ›å»ºä¸€ä¸ªç´¢å¼•å­˜å‚¨çš„ä¸Šä¸‹æ–‡å¯¹è±¡ ç”¨äºŽåŽç»­ç´¢å¼•çš„åŠ è½½ è¿™æ˜¯llama_indexè¯»å–æœ¬åœ°å‘é‡ç´¢å¼•æ—¶çš„æ ‡å‡†å†™æ³•
                    self.index = load_index_from_storage(storage_context)  # ç”¨llama_indexçš„æŽ¥å£ æŠŠæœ¬åœ°å­˜å‚¨é‡Œçš„å‘é‡ç´¢å¼•æ•°æ®åŠ è½½åˆ°å†…å­˜ å¹¶èµ‹å€¼ç»™self.index åŽç»­å°±å¯ä»¥æ£€ç´¢äº†
                    pbar.update(1)  # æ‰‹åŠ¨æŠŠè¿›åº¦æ¡æŽ¨è¿›1æ­¥ è¡¨ç¤ºâ€œåŠ è½½ç´¢å¼•â€è¿™ä¸€æ­¥å·²ç»å®Œæˆ
                load_time = time.time() - start_time
                logger.info(f"Loaded Existing Index for Global Chat Engine in {load_time:.2f} Seconds.")
            else:
                # If no documents provided and no existing index, skip building
                if not documents:  # å¦‚æžœdocumentsè¿™ä¸ªåˆ—è¡¨æœ¬èº«æ˜¯ç©ºçš„ï¼ˆæ²¡æœ‰å¯ç”¨æ–‡æ¡£ï¼‰é‚£è¿žç´¢å¼•éƒ½æ²¡æ³•å»º
                    logger.info("No Document Found and No Existing Index Found.")
                    self.index = None
                    return
                
                start_time = time.time()  ####################
                logger.info("Building New Index...")

                logger.info("Firstly, Parse Document Page.")
                # Parse document nodes åˆ‡å—æ“ä½œ
                parse_start = time.time()  ####################
                with tqdm(total=len(documents), desc="Parsing Document Pages.", unit="Document", leave=False, disable=True) as pbar:  # æ€»æ•°=æ–‡æ¡£æ•° å•ä½æ˜¯doc
                    nodes = []
                    for doc in documents:  # éåŽ†æ‰€æœ‰æ–‡æ¡£  æ‰€æœ‰åˆ‡çš„å—
                        doc_nodes = self.node_parser.get_nodes_from_documents([doc])  # æ¯ä»½æ–‡æ¡£ç”¨node_parserï¼ˆé€šå¸¸æ˜¯åˆ†å¥/åˆ†å—å™¨ï¼‰åˆ‡åˆ†æˆè‹¥å¹²â€œèŠ‚ç‚¹â€ï¼ˆchunkï¼‰
                        nodes.extend(doc_nodes)  # æŠŠè¿™äº›èŠ‚ç‚¹åˆå¹¶åˆ°nodesæ€»åˆ—è¡¨
                        pbar.update(1)  # bar+=1
                parse_time = time.time() - parse_start  #################### åˆ‡å—æ—¶é—´
                
                logger.info(f"Document Pages Split Into {len(nodes)} Nodes in {parse_time:.2f} Seconds.")  # è¿™æ¬¡æ‰€æœ‰æ–‡æ¡£æ€»å…±åˆ‡æˆå¤šå°‘ä¸ªèŠ‚ç‚¹ æ¯ä¸ªèŠ‚ç‚¹åŽç»­éƒ½ä¼šåšembedding

                # Create vector index åšembedding ç”Ÿæˆå‘é‡å¹¶å»ºç«‹ç´¢å¼•
                logger.info("Secondly, Create Index.")
                index_start = time.time()  ####################
                with tqdm(total=1, desc="Creating Vector Index.", unit="Step", leave=False, disable=True) as pbar:
                    self.index = VectorStoreIndex(nodes, show_progress=True)  # è°ƒç”¨VectorStoreIndex(nodes, ...) å¯¹æ‰€æœ‰èŠ‚ç‚¹åšembeddingå¹¶å»ºç«‹å‘é‡æ£€ç´¢ç´¢å¼• ç´¢å¼•å¯¹è±¡èµ‹å€¼ç»™self.index ä¾›åŽç»­æ£€ç´¢ç”¨
                    pbar.update(1)  # æ‰‹åŠ¨+1
                index_time = time.time() - index_start  ####################ç”Ÿæˆå‘é‡å¹¶å»ºç«‹ç´¢å¼•ç”¨äº†å¤šé•¿æ—¶é—´
                
                # Save index ä¿å­˜åˆ°æœ¬åœ°ç£ç›˜
                logger.info("Thirdly, Save Index.")
                save_start = time.time()  ####################
                with tqdm(total=1, desc="Saving Index.", unit="Step", leave=False) as pbar:
                    self.index.storage_context.persist(persist_dir=str(self.storage_dir))  # æŠŠå½“å‰ç´¢å¼•å®Œæ•´ä¿å­˜åˆ°æœ¬åœ°æŒ‡å®šç›®å½•
                    pbar.update(1)
                save_time = time.time() - save_start  ####################ä¿å­˜åˆ°æœ¬åœ°ç£ç›˜ç”¨äº†å¤šé•¿æ—¶é—´
                
                total_time = time.time() - start_time  #################### ä¸€å…±èŠ±äº†å¤šé•¿æ—¶é—´
                logger.info(f"Build Global Index Successfully.")
                logger.info(f"GLobal Index: Document Page Parsed, Vector Index Built, and Saved Successfully in {total_time:.2f} Seconds Totally.")
                logger.info(f"Parsing in {parse_time:.2f} Seconds,   Building in {index_time:.2f} Seconds,   Saving in {save_time:.2f} Seconds.")
                
        except Exception as e:
            logger.error(f"Error on Building Index: {e}.")
            raise
    
    def build_folder_indexes(self, force_rebuild: bool = False) -> None:  # åˆ†æ–‡ä»¶å¤¹æ‰¹é‡æž„å»ºç´¢å¼•çš„æ ¸å¿ƒå‡½æ•°
        """
        Build indexes grouped by folder æ‰¹é‡ä¸ºæ¯ä¸ªæ–‡ä»¶å¤¹ä¸‹çš„æ–‡æ¡£æž„å»ºå‘é‡ç´¢å¼•
        
        Args:
            force_rebuild: Whether to force rebuild indexes
        """
        try:
            overall_start_time = time.time()  # æ•´ä¸ªè¿‡ç¨‹çš„èµ·å§‹æ—¶é—´
            # Get all subfolders
            folders = []
            for item in self.documents_dir.iterdir():  # èŽ·å–æ–‡æ¡£ä¸»ç›®å½•ä¸‹çš„æ‰€æœ‰å­æ–‡ä»¶å¤¹åç§° iterdir(): åˆ—ä¸¾å½“å‰ç›®å½•ä¸‹æ‰€æœ‰çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹
                if item.is_dir():
                    folders.append(item.name)
            
            logger.info(f"Found {len(folders)} Folders: {folders}.")
            logger.info("ðŸ“Š START RECORD TIME ABOUT FOLDER INDEX.")
            
            # Statistics tracking
            folder_stats = []
            total_documents = 0
            total_nodes = 0
            successful_builds = 0
            
            # Iterate through folders with progress bar
            for folder_name in tqdm(folders, desc="Processing Folders.", unit="Folder", leave=False, disable=True):
                folder_start_time = time.time()  # è®°å½•æ¯ä¸ªæ–‡ä»¶å¤¹çš„èµ·å§‹æ—¶é—´folder_start_time
                folder_path = self.documents_dir / folder_name
                storage_path = self.storage_dir / f"index_{folder_name}"
                
                logger.info(f"ðŸ“ Processing Folder: '{folder_name}'.")
                
                # Check if saved index exists
                if storage_path.exists() and not force_rebuild:
                    load_start = time.time()  ### è®°å½•åŠ è½½çŽ°æœ‰ç´¢å¼•çš„å¼€å§‹æ—¶é—´
                    logger.info(f"Loading Existing Folder Index for Folder '{folder_name}'...")
                    try:  # å°è¯•åŠ è½½æŒ‡å®šæ–‡ä»¶å¤¹å·²æœ‰çš„å‘é‡ç´¢å¼•ï¼ˆå¦‚æžœå·²ç»æž„å»ºè¿‡ï¼‰å¹¶æŠŠç»“æžœä¿å­˜åˆ°ç³»ç»Ÿå†…å­˜çš„ç´¢å¼•å­—å…¸é‡Œ
                        storage_context = StorageContext.from_defaults(persist_dir=str(storage_path))  # åˆ›å»ºä¸€ä¸ªç´¢å¼•å­˜å‚¨çš„ä¸Šä¸‹æ–‡å¯¹è±¡ ç”¨äºŽåŽç»­ç´¢å¼•çš„åŠ è½½ è¿™æ˜¯llama_indexè¯»å–æœ¬åœ°å‘é‡ç´¢å¼•æ—¶çš„æ ‡å‡†å†™æ³•
                        self.folder_indexes[folder_name] = load_index_from_storage(storage_context)  # æŠŠç£ç›˜ä¸Šçš„å‘é‡ç´¢å¼•æ•°æ®åŠ è½½åˆ°å†…å­˜ å¹¶æ”¾è¿› self.folder_indexes è¿™ä¸ªå­—å…¸é‡Œï¼ˆä»¥æ–‡ä»¶å¤¹åä¸ºkeyï¼‰
                        load_time = time.time() - load_start  ### åŠ è½½çŽ°æœ‰ç´¢å¼•ç”¨äº†å¤šå°‘æ—¶é—´
                        folder_total_time = time.time() - folder_start_time  ### æ•´ä¸ªè¿™ä¸ªæ–‡ä»¶å¤¹çš„ç”¨æ—¶
                        
                        logger.info(f"Loaded Existing Folder Index for Folder '{folder_name}' Successfully.")
                        logger.info(f"Load Time: {load_time:.2f} Seconds,   Total Time: {folder_total_time:.2f} Seconds.")
                        
                        folder_stats.append({
                            'name': folder_name,
                            'action': 'loaded',
                            'total_time': folder_total_time,
                            'load_time': load_time,
                            'documents': 0,
                            'nodes': 0
                        })
                        successful_builds += 1
                        continue  # è·³è¿‡æœ¬è½®å¾ªçŽ¯ å·²ç»åŠ è½½å®Œæˆ ä¸éœ€è¦ç»§ç»­å¤„ç† å°±æ˜¯åŽé¢çš„ä»£ç 
                    except Exception as e:
                        logger.error(f"Failed to Load Existing Folder Index '{folder_name}', {e}.")
                
                # Load documents from this folder
                try:
                    logger.info(f"ðŸš€ Building Folder Index for Folder '{folder_name}'...")
                    doc_load_start = time.time()  ### è®°å½•æ–‡æ¡£åŠ è½½çš„èµ·å§‹æ—¶é—´
                    reader = SimpleDirectoryReader(
                        input_dir=str(folder_path),
                        recursive=True,
                        required_exts=RAGConfig.SUPPORTED_EXTENSIONS,
                    )
                    
                    documents = reader.load_data()
                    doc_load_time = time.time() - doc_load_start  ### è®°å½•æ–‡æ¡£åŠ è½½èŠ±äº†å¤šé•¿æ—¶é—´
                    
                    if not documents:  # å¦‚æžœæ²¡è¯»åˆ°æ–‡æ¡£ åˆ—è¡¨ä¸ºç©º è¾“å‡ºè­¦å‘Š
                        logger.error(f"âš ï¸ No Document Found in Folder '{folder_name}'.")
                        folder_total_time = time.time() - folder_start_time  ###
                        folder_stats.append({
                            'name': folder_name,
                            'action': 'skipped',
                            'total_time': folder_total_time,
                            'documents': 0,
                            'nodes': 0
                        })
                        continue  # è·³è¿‡è¿™æ¬¡å¾ªçŽ¯
                    
                    # Add folder identification and metadata to documents
                    for doc in documents:
                        self._add_document_metadata(doc, folder_name)  # ç»™è¿™äº›å­æ–‡ä»¶å¤¹çš„æ–‡ä»¶æ·»åŠ metadata å®šä¹‰å¥½çš„å‡½æ•°
                    
                    logger.info(f"Read {len(documents)} Document Pages in {doc_load_time:.2f} Seconds under Current Folder '{folder_name}'.")

                    # Parse document nodes
                    logger.info("Firstly, Parse Document Pages under Current Folder.")
                    parse_start = time.time()  ### è®°å½•â€œæ‹†åˆ†èŠ‚ç‚¹â€å¼€å§‹æ—¶é—´
                    with tqdm(total=len(documents), desc=f"Parsing Document Page under Folder: {folder_name}.", unit="Page", leave=False, disable=True) as pbar:  # False è¿›åº¦æ¡ç”¨å®ŒåŽè¦ä¸è¦ç•™åœ¨ç»ˆç«¯ï¼ˆæˆ–æ—¥å¿—ï¼‰é‡Œã€‚
                        nodes = []
                        for doc in documents:
                            doc_nodes = self.node_parser.get_nodes_from_documents([doc])
                            nodes.extend(doc_nodes)
                            pbar.update(1)  # è¿›åº¦æ¡å‰è¿›1æ­¥
                    parse_time = time.time() - parse_start  ### æ‹†åˆ†è¿™äº›èŠ‚ç‚¹æ‰€ç”¨æ—¶é—´
                    
                    logger.info(f"Document Pages Split Into {len(nodes)} Nodes in {parse_time:.2f} Seconds under Current Folder '{folder_name}'.")
                    
                    # Create vector index åˆ›å»ºembeddingå‘é‡ç´¢å¼•
                    logger.info("Secondly, Create Folder Index under Current Folder.")
                    index_start = time.time()
                    with tqdm(total=1, desc=f"Creating Folder Index for Folder {folder_name}.", unit="Step", leave=False, disable=True) as pbar:
                        folder_index = VectorStoreIndex(nodes, show_progress=True)
                        pbar.update(1)
                    index_time = time.time() - index_start  # è®°å½•ç´¢å¼•æž„å»ºæ‰€ç”¨çš„æ—¶é—´
                    
                    # Save index
                    logger.info("Thirdly, Save Folder Index under Current Folder.")
                    save_start = time.time()
                    with tqdm(total=1, desc=f"Saving Folder Index for Folder {folder_name}.", unit="Step", leave=False, disable=True) as pbar:
                        storage_path.mkdir(exist_ok=True)
                        folder_index.storage_context.persist(persist_dir=str(storage_path))  # æŠŠåˆšåˆšå»ºç«‹çš„ç´¢å¼•ç»“æž„ å‘é‡ å…ƒæ•°æ®ç­‰ ä¿å­˜åˆ°ç£ç›˜ä¸Š æ–¹ä¾¿ä¸‹æ¬¡ç›´æŽ¥åŠ è½½ æ— éœ€é‡å»º
                        pbar.update(1)
                    save_time = time.time() - save_start  # ä¿å­˜ç´¢å¼•è€—æ—¶
                    
                    # Store in memory
                    self.folder_indexes[folder_name] = folder_index  # åœ¨é‚£ä¸ªå­—å…¸ç»“æž„é‡Œä¿å­˜ä¸€ä¸‹
                    
                    folder_total_time = time.time() - folder_start_time  # ç»Ÿè®¡æœ¬æ¬¡æ€»ç”¨æ—¶ï¼ˆä»Žè¿™ä¸ªå­æ–‡ä»¶å¤¹å¼€å§‹å¤„ç†åˆ°çŽ°åœ¨ï¼‰

                    logger.info(f"Already Build Folder Index for Folder '{folder_name}' Successfully.")
                    logger.info(f"Total Time: {folder_total_time:.2f} Seconds,   Read Time: {doc_load_time:.2f} Seconds,   Parsed Time: {parse_time:.2f} Seconds,   Built Time: {index_time:.2f} Seconds,   Saved Time: {save_time:.2f} Seconds.")
                    
                    # Update statistics
                    total_documents += len(documents)
                    total_nodes += len(nodes)
                    successful_builds += 1
                    
                    folder_stats.append({
                        'name': folder_name,
                        'action': 'built',
                        'total_time': folder_total_time,
                        'doc_load_time': doc_load_time,
                        'parse_time': parse_time,
                        'index_time': index_time,
                        'save_time': save_time,
                        'documents': len(documents),
                        'nodes': len(nodes)
                    })
                    
                except Exception as e:
                    folder_total_time = time.time() - folder_start_time
                    logger.error(f"Error on Building Folder Index for Folder '{folder_name}': {e} (Time: {folder_total_time:.2f} Seconds).")
                    folder_stats.append({
                        'name': folder_name,
                        'action': 'failed',
                        'total_time': folder_total_time,
                        'error': str(e)
                    })
                    continue  # continue è·³è¿‡æœ¬è½® è¿›å…¥ä¸‹ä¸€ä¸ªæ–‡ä»¶å¤¹
            
            overall_time = time.time() - overall_start_time
            
            # Print detailed timing report
            logger.info("DETAILED TIMING REPORT FOR FOLDER INDEX.")
            # Sort by total time for better analysis
            folder_stats.sort(key=lambda x: x.get("total_time", 0), reverse=True)
            for stat in folder_stats:
                if stat['action'] == 'built':
                    logger.info(f"ðŸ“ Folder {stat['name']:20}, TOTAL BUILT TIME: {stat['total_time']:6.2f}s, PAGE COUNT: {stat['documents']:3d}, NODE COUNT: {stat['nodes']:5d}, READ TIME: {stat['doc_load_time']:4.2f}s, PARSE TIME: {stat['parse_time']:4.2f}s, CREATE INDEX TIME: {stat['index_time']:4.2f}s, SAVE TIME: {stat['save_time']:4.2f}s.")
                        # æœ€å°å®½åº¦ 20 ä¸ªå­—ç¬¦ï¼Œå­—ç¬¦ä¸²é»˜è®¤å·¦å¯¹é½ï¼Œä¸è¶³ç”¨ç©ºæ ¼å¡«å……ï¼›è¶…è¿‡ 20 ä¸ä¼šæˆªæ–­
                elif stat['action'] == 'failed':  # æž„å»ºå¤±è´¥ æ‰“å°é”™è¯¯å†…å®¹
                    logger.info(f"âŒ FAILED on Folder {stat['name']:20}, Running Time: {stat['total_time']:6.2f} Seconds, With Error: {stat.get('error', 'Unknown Error')}.")
                    
                elif stat['action'] == 'skipped':  # æ–‡ä»¶å¤¹æ²¡æœ‰æ–‡æ¡£ ç›´æŽ¥è·³è¿‡ è¾“å‡ºæç¤º
                    logger.info(f"â­ï¸ SKIPPED   Folder {stat['name']:20}, Running Time: {stat['total_time']:6.2f} Seconds, No Document Found.")
            
            # æ€»ç”¨æ—¶ æ–‡ä»¶å¤¹æ•° æˆåŠŸæž„å»º/åŠ è½½æ•° æ–‡æ¡£æ€»æ•° èŠ‚ç‚¹æ€»æ•° å†…å­˜é‡Œæœ€ç»ˆçš„ç´¢å¼•æ•°é‡
            logger.info("ðŸ“Š STATISTICS SUMMARY FOR FOLDER INDEX.")
            logger.info(f"âœ… Total Time Taken:                {overall_time:.2f} Seconds.")
            logger.info(f"âœ… Total Folder Processed:          {len(folders)}.")
            logger.info(f"âœ… Total Folder Index Built/Loaded: {successful_builds}.")
            logger.info(f"âœ… Total Folder Index:              {len(self.folder_indexes)}.")
            logger.info(f"âœ… Total Document Page Processed:   {total_documents}.")
            logger.info(f"âœ… Total Node Created:              {total_nodes}.")
            
            
        except Exception as e:
            logger.error(f"Error on Building Folder Index: {e}.")
            raise
    
    def create_chat_engine(self, similarity_top_k: Optional[int] = None):
        """
        Create chat engine
        
        Args:
            similarity_top_k: Number of similar documents to retrieve
        """
        if self.index is None:
            raise ValueError("Please Build Global Chat Engine Index First.")
        
        if similarity_top_k is None:
            similarity_top_k = RAGConfig.SIMILARITY_TOP_K
        
        try:
            # Create chat memory
            memory = ChatMemoryBuffer.from_defaults(token_limit=RAGConfig.TOKEN_LIMIT)
            
            # Create chat engine
            self.chat_engine = self.index.as_chat_engine(
                chat_mode="context",  # ç”¨ä¸Šä¸‹æ–‡æ¨¡å¼ æ¯æ¬¡å¯¹è¯éƒ½ç»“åˆåŽ†å²å’Œæ£€ç´¢å†…å®¹
                memory=memory,
                similarity_top_k=similarity_top_k,
                system_prompt=SystemPrompts.DEFAULT,  # æ ‡å‡†æç¤ºè¯ ä¸æ˜¯æ¨¡ç‰ˆçš„é‚£ä¸ª
                verbose=True #  è¯¦ç»†è¾“å‡ºå†…éƒ¨æ—¥å¿— ä¾¿äºŽè°ƒè¯•
            )
            logger.info(f"Global Chat Engine Created Successfully with Maximal {RAGConfig.TOKEN_LIMIT} MEMORY Tokens.")
            
        except Exception as e:
            logger.error(f"Error on Creating Global Chat Engine: {e}.")
            raise
    
    def create_folder_chat_engine(self, folder_name: str, similarity_top_k: Optional[int] = None):  # ä¸ºæŒ‡å®šæ–‡ä»¶å¤¹åˆ›å»ºä¸€ä¸ªâ€œä¸“å±žèŠå¤©å¼•æ“Žâ€
        """
        Create chat engine for specified folder
        
        Args:
            folder_name: Folder name
            similarity_top_k: Number of similar documents to retrieve
        """
        if folder_name not in self.folder_indexes:
            raise ValueError(f"Folder Index for Folder '{folder_name}' Not Found.")
        
        if similarity_top_k is None:
            similarity_top_k = RAGConfig.SIMILARITY_TOP_K
        
        try:
            # Create chat memory
            memory = ChatMemoryBuffer.from_defaults(token_limit=RAGConfig.TOKEN_LIMIT)
            
            # Create chat engine
            chat_engine = self.folder_indexes[folder_name].as_chat_engine(
                chat_mode="context",
                memory=memory,
                similarity_top_k=similarity_top_k,
                system_prompt=SystemPrompts.folder_specific(folder_name),
                verbose=True
            )

            self.folder_chat_engines[folder_name] = chat_engine
            logger.info(f"ðŸš€ Expert Chat Engine for Folder '{folder_name}' Created Successfully.")
            
        except Exception as e:
            logger.error(f"Error on Creating Expert Chat Engine for Folder '{folder_name}'; {e}.")
            raise
    
    def chat_with_folder(self, folder_name: str, message: str) -> str:  # å®žé™…å’ŒæŸä¸ªæ–‡ä»¶å¤¹çš„çŸ¥è¯†åº“å¯¹è¯
        """
        Chat with documents in specified folder
        
        Args:
            folder_name: Folder name
            message: User message
            
        Returns:
            AI response
        """
        if folder_name not in self.folder_chat_engines:
            # If chat engine doesn't exist, try to create it
            if folder_name in self.folder_indexes:
                self.create_folder_chat_engine(folder_name)
            else:
                raise ValueError(f"Folder Index for Folder '{folder_name}' Not Found.")
        
        try:
            response = self.folder_chat_engines[folder_name].chat(message)
            return str(response)
        except Exception as e:
            logger.error(f"Error During Chat under Folder '{folder_name}'; {e}.")
            return f"Sorry, Error Occurred When Processing the Question; {e}."
    
    def get_available_folders(self) -> List[str]:
        """
        Get list of available folders
        
        Returns:
            List of folder names
        """
        return list(self.folder_indexes.keys())
    
    def get_folder_documents_count(self, folder_name: str) -> int:
        """
        Get document count for specified folder
        
        Args:
            folder_name: Folder name
            
        Returns:
            Document count
        """
        try:
            folder_path = self.documents_dir / folder_name
            if not folder_path.exists() or not folder_path.is_dir():
                return 0  # å¦‚æžœè¯¥è·¯å¾„ä¸å­˜åœ¨ æˆ–è€…ä¸æ˜¯æ–‡ä»¶å¤¹ ç›´æŽ¥è¿”å›ž0ï¼ˆæ²¡æœ‰ä»»ä½•æ–‡ä»¶ï¼‰
            
            count = 0
            for file_path in folder_path.rglob("*"):  # ç”¨rglob("*")é€’å½’éåŽ†è¯¥æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰å±‚çº§çš„æ–‡ä»¶å’Œå­æ–‡ä»¶å¤¹
                if file_path.is_file():
                    count += 1  # å¦‚æžœéåŽ†åˆ°çš„æ˜¯â€œæ–‡ä»¶â€ï¼Œå°±è®¡æ•°å™¨åŠ 1
            return count
        except Exception as e:
            logger.error(f"Error on Getting Document File Count in Folder '{folder_name}'; {e}.")
            return 0
    
    def chat(self, message: str) -> str:
        """
        Chat with the system
        
        Args:
            message: User message
            
        Returns:
            System response
        """
        if self.chat_engine is None:
            raise ValueError("Please Create Global Chat Engine First.")
        
        try:
            response = self.chat_engine.chat(message)
            return str(response)
        except Exception as e:
            logger.error(f"Error During Chat: {e}.")
            return f"Sorry, Error Occurred When Processing the Question; {e}."
        