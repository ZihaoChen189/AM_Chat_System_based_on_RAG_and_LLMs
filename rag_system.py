import logging
import random
import shutil
import time
from typing import List, Optional, Dict
from pathlib import Path
from tqdm import tqdm
import torch
import numpy as np

from llama_index.core import (
    VectorStoreIndex,  # åˆ›å»ºå‘é‡ç´¢å¼•å¯¹è±¡
    SimpleDirectoryReader,
    Document,
    StorageContext,  # ç´¢å¼•æ•°æ®è¦å­˜åœ¨å“ªé‡Œ æ€ä¹ˆå­˜ ç”¨ä»€ä¹ˆæ–¹å¼å­˜
    load_index_from_storage,  # ç”¨äºä»æœ¬åœ°åŠ è½½å·²æœ‰çš„ç´¢å¼• è¦ä¼ å…¥ä¸€ä¸ªStorageContextå¯¹è±¡
    Settings
)

from llama_index.core.node_parser import SentenceSplitter  # node_parserç”¨åˆ°äº†
from llama_index.llms.openai_like import OpenAILike
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.memory import ChatMemoryBuffer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGConfig:
    # Document processing configuration
    CHUNK_SIZE = 512
    CHUNK_OVERLAP = 50
    SUPPORTED_EXTENSIONS = [".pdf", ".docx", ".doc", ".txt", ".md", ".html"]
    
    # LLM configuration
    DEFAULT_TEMPERATURE = 0.7
    DEFAULT_MAX_TOKENS = 2048  # é™åˆ¶LLMç”Ÿæˆçš„æœ€å¤§tokenæ•°
    
    # Retrieval configuration
    SIMILARITY_TOP_K = 3
    TOKEN_LIMIT = 3000  # å¤šè½®å¯¹è¯æ—¶ ä¿å­˜å¤šå°‘tokençš„å†å²
    
    # Random seed
    RANDOM_SEED = 42


class SystemPrompts:
    DEFAULT = (
        "You are an intelligent assistant that answers user questions based on provided document content."
        "Please answer based on the retrieved relevant document content. If there is no relevant information in the documents, please clearly state so."
        "When answering, please be accurate, detailed, and helpful. If possible, please cite specific document sources."
    )
# Answer ONLY with the facts listed in the sources below.
# If there isnâ€™t enough information below, say you donâ€™t know. DO NOT generate answers that donâ€™t use the sources below. 
# If asking a clarifying question to the user would help, ask the question.
# If the question is not in English, translate the question to English before generating the search query.

    @staticmethod
    def folder_specific(folder_name: str) -> str:
        return (
            f"You are an intelligent assistant that specializes in answering user questions based on document content from the '{folder_name}' folder."
            "Please answer based on the retrieved relevant document content. If there is no relevant information in the documents, please clearly state so."
            "When answering, please be accurate, detailed, and helpful. If possible, please cite specific document sources."
        )


class MultiModelRAGSystem:
    def __init__(self, api_key: str, model_type: str = "deepseek", documents_dir: str = "./documents", storage_dir: str = "./storage", embedding_model: str = "BAAI/bge-small-zh-v1.5", random_seed: Optional[int] = None):
        # Set random seed for reproducibility
        self.random_seed = random_seed or RAGConfig.RANDOM_SEED  # å¦‚æœinitæ²¡ä¼  æœ‰é»˜è®¤çš„
        self._set_random_seeds()  # defined function below
        
        self.api_key = api_key
        self.model_type = model_type.lower()
        self.documents_dir = Path(documents_dir)
        self.storage_dir = Path(storage_dir)
        
        # Create directories
        self.documents_dir.mkdir(exist_ok=True)
        self.storage_dir.mkdir(exist_ok=True)
        
        # Configure LLM based on model type
        self.llm = self._configure_llm()  # åˆå§‹åŒ–LLMå¯¹è±¡ # defined function below
        
        logger.info(f"Using model: {self.model_type}")
        logger.info(f"Random seed: {self.random_seed}")
        
        # Configure embedding model (using local model with CUDA acceleration)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Embedding model using device: {device}")
        
        self.embed_model = HuggingFaceEmbedding(
            model_name=embedding_model,
            cache_folder="./embedding_cache",
            device=device
        )
        
        # Configure global settings
        Settings.llm = self.llm
        Settings.embed_model = self.embed_model
        Settings.chunk_size = RAGConfig.CHUNK_SIZE
        Settings.chunk_overlap = RAGConfig.CHUNK_OVERLAP
        
        # Configure node parser
        self.node_parser = SentenceSplitter(
            chunk_size=RAGConfig.CHUNK_SIZE,
            chunk_overlap=RAGConfig.CHUNK_OVERLAP,
        )
        
        self.index = None
        self.chat_engine = None
        
        # Folder-based indexing functionality
        self.folder_indexes: Dict[str, VectorStoreIndex] = {}
        self.folder_chat_engines: Dict[str, any] = {}
    
    def _set_random_seeds(self) -> None:
        """Set random seeds for reproducibility"""
        random.seed(self.random_seed)
        np.random.seed(self.random_seed)
        torch.manual_seed(self.random_seed)  # PyTorchçš„CPUéšæœºæ•°ç”Ÿæˆå™¨
        if torch.cuda.is_available():  # GPU
            torch.cuda.manual_seed(self.random_seed)
            torch.cuda.manual_seed_all(self.random_seed)
        # Set deterministic behavior
        # è¿™ä¸¤è¡Œç”¨äºå½»åº•ä¿è¯æ“ä½œçš„ç¡®å®šæ€§ æœ‰çš„åº•å±‚å®ç°æœ‰â€œéç¡®å®šæ€§åŠ é€Ÿä¼˜åŒ–â€ è¿™æ ·å†™èƒ½å…³æ‰æ‰€æœ‰ä¸ç¡®å®šæ€§æ“ä½œ ç‰ºç‰²ä¸€ç‚¹é€Ÿåº¦æ¢æ¥â€œå®Œå…¨å¯å¤ç°â€
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        logger.info(f"Random seed set: {self.random_seed}")
    
    def _get_base_llm_config(self) -> dict:
        """Get base LLM configuration"""
        # ä¸“é—¨è´Ÿè´£åŸºç¡€å…¬å…±é…ç½® api_key temperature max_tokens è¿™äº›å‚æ•°éƒ½é€šç”¨
        return {
            'api_key': self.api_key,
            'temperature': RAGConfig.DEFAULT_TEMPERATURE,
            'max_tokens': RAGConfig.DEFAULT_MAX_TOKENS,
            'is_chat_model': True,  # å£°æ˜è¿™æ˜¯â€œå¯¹è¯å¤§æ¨¡å‹â€
        }
    
    def _configure_llm(self):  # æ ¹æ®é€‰çš„æ¨¡å‹ç±»å‹ åŠ¨æ€åˆæˆå…¨é‡é…ç½® å¹¶åˆ›å»ºæ¨¡å‹å¯¹è±¡
        """
        Configure LLM based on model type
        
        Returns:
            Configured LLM instance
        """
        base_config = self._get_base_llm_config()
        
        model_configs = {
            "deepseek": {
                "model": "deepseek-chat",  # DeepSeek-V3-0324 Jul-16
                "api_base": "https://api.deepseek.com/v1",  # DeepSeek-R1-0528 JUl-16
            },
            "deepseek-reasoner": {
                "model": "deepseek-reasoner",
                "api_base": "https://api.deepseek.com/v1",
            },
            "gpt4.1": {
                "model": "gpt-4-turbo-preview",
                "api_base": "https://api.openai.com/v1",
            },
            "gpt4o": {
                "model": "gpt-4o",
                "api_base": "https://api.openai.com/v1",
            }
        }
        
        if self.model_type not in model_configs:
            raise ValueError(f"Unsupported model type: {self.model_type}. Supported types: {', '.join(model_configs.keys())}")
        # base_config æ˜¯é€šç”¨çš„LLMé…ç½®ä¿¡æ¯;
        # model_configs æ˜¯å…·ä½“LLMå“ç‰Œçš„é…ç½®ä¿¡æ¯
        config = {**base_config, **model_configs[self.model_type]}
        return OpenAILike(**config)  # å‚æ•°è§£åŒ… Keyword Arguments unpacking
        
    def _add_document_metadata(self, doc: Document, folder_name: Optional[str] = None) -> None:
        """Add standard metadata to document"""
        # hasattr æ˜¯å¦æœ‰æŸä¸ªå±æ€§
        if hasattr(doc, 'metadata') and 'file_path' in doc.metadata:
            file_path = Path(doc.metadata['file_path'])
            doc.metadata.update({
                'filename': file_path.name,
                'file_type': file_path.suffix,
                'file_size': file_path.stat().st_size if file_path.exists() else 0
            })
            if folder_name:
                doc.metadata['folder'] = folder_name
                
    def load_documents(self, file_extensions: Optional[List[str]] = None) -> List[Document]:
        """
        Load documents from specified directory
        
        Args:
            file_extensions: List of supported file extensions
            
        Returns:
            List of documents
        """
        if file_extensions is None:
            file_extensions = RAGConfig.SUPPORTED_EXTENSIONS
        
        try:
            reader = SimpleDirectoryReader(
                input_dir=str(self.documents_dir),
                recursive=True,
                required_exts=file_extensions,
            )
            
            documents = reader.load_data(show_progress=True,num_workers=4)
            logger.info(f"Successfully loaded {len(documents)} document pages")  # å¤šå°‘page
            
            # Add metadata to each document
            for doc in documents:
                self._add_document_metadata(doc)  # ç»™æ¯ä¸€ä¸ªdocæ·»åŠ å…ƒæ•°æ®
            
            return documents
            
        except Exception as e:
            logger.error(f"Error loading documents: {e}")
            return []
    
    def build_index(self, documents: List[Document], force_rebuild: bool = False) -> None:
        """
        Build or load vector index
        
        Args:
            documents: List of documents
            force_rebuild: Whether to force rebuild index
        """
        index_store_file = self.storage_dir / "index_store.json"
        docstore_file = self.storage_dir / "docstore.json"
        
        try:
            # Check if saved index exists
            if index_store_file.exists() and docstore_file.exists() and not force_rebuild:
                start_time = time.time()
                logger.info("Loading existing index...")

                # StorageContext,  # ç´¢å¼•æ•°æ®è¦å­˜åœ¨å“ªé‡Œ æ€ä¹ˆå­˜ ç”¨ä»€ä¹ˆæ–¹å¼å­˜
                # load_index_from_storage,  # ç”¨äºä»æœ¬åœ°åŠ è½½å·²æœ‰çš„ç´¢å¼• è¦ä¼ å…¥ä¸€ä¸ªStorageContextå¯¹è±¡

                with tqdm(total=1, desc="Loading index", unit="step") as pbar:
                    storage_context = StorageContext.from_defaults(persist_dir=str(self.storage_dir))  # åˆ›å»ºä¸€ä¸ªç´¢å¼•å­˜å‚¨çš„ä¸Šä¸‹æ–‡å¯¹è±¡ ç”¨äºåç»­ç´¢å¼•çš„åŠ è½½ è¿™æ˜¯llama_indexè¯»å–æœ¬åœ°å‘é‡ç´¢å¼•æ—¶çš„æ ‡å‡†å†™æ³•
                    self.index = load_index_from_storage(storage_context)  # ç”¨llama_indexçš„æ¥å£ æŠŠæœ¬åœ°å­˜å‚¨é‡Œçš„å‘é‡ç´¢å¼•æ•°æ®åŠ è½½åˆ°å†…å­˜ å¹¶èµ‹å€¼ç»™self.index åç»­å°±å¯ä»¥æ£€ç´¢äº†
                    pbar.update(1)  # æ‰‹åŠ¨æŠŠè¿›åº¦æ¡æ¨è¿›1æ­¥ è¡¨ç¤ºâ€œåŠ è½½ç´¢å¼•â€è¿™ä¸€æ­¥å·²ç»å®Œæˆ
                load_time = time.time() - start_time
                logger.info(f"Index loading completed in {load_time:.2f} seconds")
            else:
                # If no documents provided and no existing index, skip building
                if not documents:  # å¦‚æœdocumentsè¿™ä¸ªåˆ—è¡¨æœ¬èº«æ˜¯ç©ºçš„ï¼ˆæ²¡æœ‰å¯ç”¨æ–‡æ¡£ï¼‰é‚£è¿ç´¢å¼•éƒ½æ²¡æ³•å»º
                    logger.info("No documents provided and no existing index found, skipping index building")
                    self.index = None
                    return
                
                start_time = time.time()  ####################
                logger.info("Building new vector index...")
                
                # Parse document nodes åˆ‡å—æ“ä½œ
                parse_start = time.time()  ####################
                with tqdm(total=len(documents), desc="Parsing documents", unit="doc") as pbar:  # æ€»æ•°=æ–‡æ¡£æ•° å•ä½æ˜¯doc
                    nodes = []
                    for doc in documents:  # éå†æ‰€æœ‰æ–‡æ¡£  æ‰€æœ‰åˆ‡çš„å—
                        doc_nodes = self.node_parser.get_nodes_from_documents([doc])  # æ¯ä»½æ–‡æ¡£ç”¨node_parserï¼ˆé€šå¸¸æ˜¯åˆ†å¥/åˆ†å—å™¨ï¼‰åˆ‡åˆ†æˆè‹¥å¹²â€œèŠ‚ç‚¹â€ï¼ˆchunkï¼‰
                        nodes.extend(doc_nodes)  # æŠŠè¿™äº›èŠ‚ç‚¹åˆå¹¶åˆ°nodesæ€»åˆ—è¡¨
                        pbar.update(1)  # bar+=1
                parse_time = time.time() - parse_start  #################### åˆ‡å—æ—¶é—´
                
                logger.info(f"Documents split into {len(nodes)} nodes in {parse_time:.2f} seconds")  # è¿™æ¬¡æ‰€æœ‰æ–‡æ¡£æ€»å…±åˆ‡æˆå¤šå°‘ä¸ªèŠ‚ç‚¹ æ¯ä¸ªèŠ‚ç‚¹åç»­éƒ½ä¼šåšembedding
                
                # Create vector index åšembedding ç”Ÿæˆå‘é‡å¹¶å»ºç«‹ç´¢å¼•
                index_start = time.time()  ####################
                with tqdm(total=1, desc="Building vector index", unit="step") as pbar:
                    self.index = VectorStoreIndex(nodes, show_progress=True)  # è°ƒç”¨VectorStoreIndex(nodes, ...) å¯¹æ‰€æœ‰èŠ‚ç‚¹åšembeddingå¹¶å»ºç«‹å‘é‡æ£€ç´¢ç´¢å¼• ç´¢å¼•å¯¹è±¡èµ‹å€¼ç»™self.index ä¾›åç»­æ£€ç´¢ç”¨
                    pbar.update(1)  # æ‰‹åŠ¨+1
                index_time = time.time() - index_start  ####################ç”Ÿæˆå‘é‡å¹¶å»ºç«‹ç´¢å¼•ç”¨äº†å¤šé•¿æ—¶é—´
                
                # Save index ä¿å­˜åˆ°æœ¬åœ°ç£ç›˜
                save_start = time.time()  ####################
                with tqdm(total=1, desc="Saving index", unit="step") as pbar:
                    self.index.storage_context.persist(persist_dir=str(self.storage_dir))  # æŠŠå½“å‰ç´¢å¼•å®Œæ•´ä¿å­˜åˆ°æœ¬åœ°æŒ‡å®šç›®å½•
                    pbar.update(1)
                save_time = time.time() - save_start  ####################ä¿å­˜åˆ°æœ¬åœ°ç£ç›˜ç”¨äº†å¤šé•¿æ—¶é—´
                
                total_time = time.time() - start_time  #################### ä¸€å…±èŠ±äº†å¤šé•¿æ—¶é—´
                logger.info(f"Vector index built and saved successfully in {total_time:.2f} seconds")
                logger.info(f"  - Parsing: {parse_time:.2f}s, Building: {index_time:.2f}s, Saving: {save_time:.2f}s")
                
        except Exception as e:
            logger.error(f"Error building index: {e}")
            raise
    
    def build_folder_indexes(self, force_rebuild: bool = False) -> None:  # åˆ†æ–‡ä»¶å¤¹æ‰¹é‡æ„å»ºç´¢å¼•çš„æ ¸å¿ƒå‡½æ•°
        """
        Build indexes grouped by folder æ‰¹é‡ä¸ºæ¯ä¸ªæ–‡ä»¶å¤¹ä¸‹çš„æ–‡æ¡£æ„å»ºå‘é‡ç´¢å¼•
        
        Args:
            force_rebuild: Whether to force rebuild indexes
        """
        try:
            overall_start_time = time.time()  # æ•´ä¸ªè¿‡ç¨‹çš„èµ·å§‹æ—¶é—´
            # Get all subfolders
            folders = []  # æ‰€æœ‰è‡ªæ–‡ä»¶å¤¹çš„åå­—
            for item in self.documents_dir.iterdir():  # è·å–æ–‡æ¡£ä¸»ç›®å½•ä¸‹çš„æ‰€æœ‰å­æ–‡ä»¶å¤¹åç§° iterdir(): åˆ—ä¸¾å½“å‰ç›®å½•ä¸‹æ‰€æœ‰çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹
                if item.is_dir():
                    folders.append(item.name)
            
            logger.info(f"Found {len(folders)} folders: {folders}")
            logger.info("=" * 80)
            logger.info("ğŸ“Š FOLDER INDEX BUILD TIMING REPORT")
            logger.info("=" * 80)
            
            # Statistics tracking
            folder_stats = []
            total_documents = 0
            total_nodes = 0
            successful_builds = 0
            
            # Iterate through folders with progress bar
            for folder_name in tqdm(folders, desc="Processing folders", unit="folder"):
                folder_start_time = time.time()  # è®°å½•æ¯ä¸ªæ–‡ä»¶å¤¹çš„èµ·å§‹æ—¶é—´folder_start_time
                folder_path = self.documents_dir / folder_name
                storage_path = self.storage_dir / f"index_{folder_name}"
                
                logger.info(f"\nğŸ“ Processing folder: '{folder_name}'")
                
                # Check if saved index exists
                if storage_path.exists() and not force_rebuild:
                    load_start = time.time()  ### è®°å½•åŠ è½½ç°æœ‰ç´¢å¼•çš„å¼€å§‹æ—¶é—´
                    logger.info(f"  â³ Loading existing index for folder '{folder_name}'...")
                    try:  # å°è¯•åŠ è½½æŒ‡å®šæ–‡ä»¶å¤¹å·²æœ‰çš„å‘é‡ç´¢å¼•ï¼ˆå¦‚æœå·²ç»æ„å»ºè¿‡ï¼‰å¹¶æŠŠç»“æœä¿å­˜åˆ°ç³»ç»Ÿå†…å­˜çš„ç´¢å¼•å­—å…¸é‡Œ
                        storage_context = StorageContext.from_defaults(persist_dir=str(storage_path))  # åˆ›å»ºä¸€ä¸ªç´¢å¼•å­˜å‚¨çš„ä¸Šä¸‹æ–‡å¯¹è±¡ ç”¨äºåç»­ç´¢å¼•çš„åŠ è½½ è¿™æ˜¯llama_indexè¯»å–æœ¬åœ°å‘é‡ç´¢å¼•æ—¶çš„æ ‡å‡†å†™æ³•
                        self.folder_indexes[folder_name] = load_index_from_storage(storage_context)  # æŠŠç£ç›˜ä¸Šçš„å‘é‡ç´¢å¼•æ•°æ®åŠ è½½åˆ°å†…å­˜ å¹¶æ”¾è¿› self.folder_indexes è¿™ä¸ªå­—å…¸é‡Œï¼ˆä»¥æ–‡ä»¶å¤¹åä¸ºkeyï¼‰
                        load_time = time.time() - load_start  ### åŠ è½½ç°æœ‰ç´¢å¼•ç”¨äº†å¤šå°‘æ—¶é—´
                        folder_total_time = time.time() - folder_start_time  ### æ•´ä¸ªè¿™ä¸ªæ–‡ä»¶å¤¹çš„ç”¨æ—¶
                        
                        logger.info(f"  âœ… Index for folder '{folder_name}' loaded successfully")
                        logger.info(f"  â±ï¸  Load time: {load_time:.2f}s, Total time: {folder_total_time:.2f}s")
                        
                        folder_stats.append({
                            'name': folder_name,
                            'action': 'loaded',
                            'total_time': folder_total_time,
                            'load_time': load_time,
                            'documents': 0,
                            'nodes': 0
                        })
                        successful_builds += 1
                        continue  # è·³è¿‡æœ¬è½®å¾ªç¯ å·²ç»åŠ è½½å®Œæˆ ä¸éœ€è¦ç»§ç»­å¤„ç† å°±æ˜¯åé¢çš„ä»£ç 
                    except Exception as e:
                        logger.warning(f"  âš ï¸  Failed to load index for folder '{folder_name}', will rebuild: {e}")
                
                # Load documents from this folder
                try:
                    doc_load_start = time.time()  ### è®°å½•æ–‡æ¡£åŠ è½½çš„èµ·å§‹æ—¶é—´
                    reader = SimpleDirectoryReader(
                        input_dir=str(folder_path),
                        recursive=True,
                        required_exts=RAGConfig.SUPPORTED_EXTENSIONS,
                    )
                    
                    documents = reader.load_data()
                    doc_load_time = time.time() - doc_load_start  ### è®°å½•æ–‡æ¡£åŠ è½½èŠ±äº†å¤šé•¿æ—¶é—´
                    
                    if not documents:  # å¦‚æœæ²¡è¯»åˆ°æ–‡æ¡£ åˆ—è¡¨ä¸ºç©º è¾“å‡ºè­¦å‘Š
                        logger.warning(f"  âš ï¸  No valid documents found in folder '{folder_name}'")
                        folder_total_time = time.time() - folder_start_time  ###
                        folder_stats.append({
                            'name': folder_name,
                            'action': 'skipped',
                            'total_time': folder_total_time,
                            'documents': 0,
                            'nodes': 0
                        })
                        continue  # è·³è¿‡è¿™æ¬¡å¾ªç¯
                    
                    # Add folder identification and metadata to documents
                    for doc in documents:
                        self._add_document_metadata(doc, folder_name)  # ç»™è¿™äº›å­æ–‡ä»¶å¤¹çš„æ–‡ä»¶æ·»åŠ metadata å®šä¹‰å¥½çš„å‡½æ•°
                    
                    logger.info(f"  ğŸ“„ Found {len(documents)} documents (loaded in {doc_load_time:.2f}s)")
                    logger.info(f"  ğŸ”¨ Building index for folder '{folder_name}'...")
                    
                    # Parse document nodes
                    parse_start = time.time()  ### è®°å½•â€œæ‹†åˆ†èŠ‚ç‚¹â€å¼€å§‹æ—¶é—´
                    with tqdm(total=len(documents), desc=f"Parsing {folder_name}", unit="page", leave=False) as pbar:  # False è¿›åº¦æ¡ç”¨å®Œåè¦ä¸è¦ç•™åœ¨ç»ˆç«¯ï¼ˆæˆ–æ—¥å¿—ï¼‰é‡Œã€‚
                        nodes = []
                        for doc in documents:
                            doc_nodes = self.node_parser.get_nodes_from_documents([doc])
                            nodes.extend(doc_nodes)
                            pbar.update(1)  # è¿›åº¦æ¡å‰è¿›1æ­¥
                    parse_time = time.time() - parse_start  ### æ‹†åˆ†è¿™äº›èŠ‚ç‚¹æ‰€ç”¨æ—¶é—´
                    
                    logger.info(f"  ğŸ“ Split into {len(nodes)} nodes (parsing: {parse_time:.2f}s)")
                    
                    # Create vector index åˆ›å»ºembeddingå‘é‡ç´¢å¼•
                    index_start = time.time()
                    with tqdm(total=1, desc=f"Building {folder_name} index", unit="step", leave=False) as pbar:
                        folder_index = VectorStoreIndex(nodes, show_progress=True)
                        pbar.update(1)
                    index_time = time.time() - index_start  # è®°å½•ç´¢å¼•æ„å»ºæ‰€ç”¨çš„æ—¶é—´
                    
                    # Save index
                    save_start = time.time()
                    with tqdm(total=1, desc=f"Saving {folder_name} index", unit="step", leave=False) as pbar:
                        storage_path.mkdir(exist_ok=True)
                        folder_index.storage_context.persist(persist_dir=str(storage_path))  # æŠŠåˆšåˆšå»ºç«‹çš„ç´¢å¼•ç»“æ„ å‘é‡ å…ƒæ•°æ®ç­‰ ä¿å­˜åˆ°ç£ç›˜ä¸Š æ–¹ä¾¿ä¸‹æ¬¡ç›´æ¥åŠ è½½ æ— éœ€é‡å»º
                        pbar.update(1)
                    save_time = time.time() - save_start  # ä¿å­˜ç´¢å¼•è€—æ—¶
                    
                    # Store in memory
                    self.folder_indexes[folder_name] = folder_index  # åœ¨é‚£ä¸ªå­—å…¸ç»“æ„é‡Œä¿å­˜ä¸€ä¸‹
                    
                    folder_total_time = time.time() - folder_start_time  # ç»Ÿè®¡æœ¬æ¬¡æ€»ç”¨æ—¶ï¼ˆä»è¿™ä¸ªå­æ–‡ä»¶å¤¹å¼€å§‹å¤„ç†åˆ°ç°åœ¨ï¼‰
                    
                    logger.info(f"  âœ… Index for folder '{folder_name}' built successfully")
                    logger.info(f"  â±ï¸  Total: {folder_total_time:.2f}s | Doc Load: {doc_load_time:.2f}s | Parse: {parse_time:.2f}s | Build: {index_time:.2f}s | Save: {save_time:.2f}s")
                    
                    # Update statistics
                    total_documents += len(documents)
                    total_nodes += len(nodes)
                    successful_builds += 1
                    
                    folder_stats.append({
                        'name': folder_name,
                        'action': 'built',
                        'total_time': folder_total_time,
                        'doc_load_time': doc_load_time,
                        'parse_time': parse_time,
                        'index_time': index_time,
                        'save_time': save_time,
                        'documents': len(documents),
                        'nodes': len(nodes)
                    })
                    
                except Exception as e:
                    folder_total_time = time.time() - folder_start_time
                    logger.error(f"  âŒ Error building index for folder '{folder_name}' (time: {folder_total_time:.2f}s): {e}")
                    folder_stats.append({
                        'name': folder_name,
                        'action': 'failed',
                        'total_time': folder_total_time,
                        'error': str(e)
                    })
                    continue  # continue è·³è¿‡æœ¬è½® è¿›å…¥ä¸‹ä¸€ä¸ªæ–‡ä»¶å¤¹
            
            overall_time = time.time() - overall_start_time
            
            # Print detailed timing report
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ“ˆ DETAILED TIMING REPORT")
            logger.info("=" * 80)
            
            # Sort by total time for better analysis
            folder_stats.sort(key=lambda x: x.get('total_time', 0), reverse=True)
            
            for stat in folder_stats:
                if stat['action'] == 'built':
                    logger.info(f"ğŸ“ {stat['name']:20} | ğŸ”¨ BUILT   | {stat['total_time']:6.2f}s | Docs: {stat['documents']:3d} | Nodes: {stat['nodes']:5d}")
                    logger.info(f"   {'':20} |          | Load: {stat['doc_load_time']:4.2f}s | Parse: {stat['parse_time']:4.2f}s | Build: {stat['index_time']:4.2f}s | Save: {stat['save_time']:4.2f}s")
                elif stat['action'] == 'loaded':  # ä»…åŠ è½½äº†å·²æœ‰ç´¢å¼• åªéœ€è¦æ˜¾ç¤ºåŠ è½½ç”¨æ—¶
                    logger.info(f"ğŸ“ {stat['name']:20} | ğŸ“‚ LOADED | {stat['total_time']:6.2f}s | Load: {stat['load_time']:4.2f}s")
                elif stat['action'] == 'failed':  # æ„å»ºå¤±è´¥ æ‰“å°é”™è¯¯å†…å®¹
                    logger.info(f"ğŸ“ {stat['name']:20} | âŒ FAILED  | {stat['total_time']:6.2f}s | Error: {stat.get('error', 'Unknown')}")
                elif stat['action'] == 'skipped':  # æ–‡ä»¶å¤¹æ²¡æœ‰æ–‡æ¡£ ç›´æ¥è·³è¿‡ è¾“å‡ºæç¤º
                    logger.info(f"ğŸ“ {stat['name']:20} | â­ï¸  SKIPPED | {stat['total_time']:6.2f}s | No documents found")
            
            # æ€»ç”¨æ—¶ æ–‡ä»¶å¤¹æ•° æˆåŠŸæ„å»º/åŠ è½½æ•° æ–‡æ¡£æ€»æ•° èŠ‚ç‚¹æ€»æ•° å†…å­˜é‡Œæœ€ç»ˆçš„ç´¢å¼•æ•°é‡
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ“Š SUMMARY STATISTICS")
            logger.info("=" * 80)
            logger.info(f"ğŸ•’ Total processing time: {overall_time:.2f} seconds")
            logger.info(f"ğŸ“ Total folders processed: {len(folders)}")
            logger.info(f"âœ… Successful builds/loads: {successful_builds}")
            logger.info(f"ğŸ“„ Total documents processed: {total_documents}")
            logger.info(f"ğŸ“ Total nodes created: {total_nodes}")
            logger.info(f"ğŸ—ï¸  Final folder indexes: {len(self.folder_indexes)}")
            
            if total_documents > 0:
                avg_time_per_doc = overall_time / total_documents
                logger.info(f"âš¡ Average time per document: {avg_time_per_doc:.3f} seconds")
            
            if total_nodes > 0:
                avg_time_per_node = overall_time / total_nodes
                logger.info(f"âš¡ Average time per node: {avg_time_per_node:.4f} seconds")
            
            logger.info("=" * 80)
            
        except Exception as e:
            logger.error(f"Error building folder indexes: {e}")
            raise
    
    def create_chat_engine(self, similarity_top_k: Optional[int] = None):
        """
        Create chat engine
        
        Args:
            similarity_top_k: Number of similar documents to retrieve
        """
        if self.index is None:
            raise ValueError("Please build index first")
        
        if similarity_top_k is None:
            similarity_top_k = RAGConfig.SIMILARITY_TOP_K
        
        try:
            # Create chat memory
            memory = ChatMemoryBuffer.from_defaults(token_limit=RAGConfig.TOKEN_LIMIT)
            
            # Create chat engine
            self.chat_engine = self.index.as_chat_engine(
                chat_mode="context",  # ç”¨ä¸Šä¸‹æ–‡æ¨¡å¼ æ¯æ¬¡å¯¹è¯éƒ½ç»“åˆå†å²å’Œæ£€ç´¢å†…å®¹
                memory=memory,
                similarity_top_k=similarity_top_k,
                system_prompt=SystemPrompts.DEFAULT,  # æ ‡å‡†æç¤ºè¯ ä¸æ˜¯æ¨¡ç‰ˆçš„é‚£ä¸ª
                verbose=True #  è¯¦ç»†è¾“å‡ºå†…éƒ¨æ—¥å¿— ä¾¿äºè°ƒè¯•
            )
            
            logger.info("Chat engine created successfully")
            
        except Exception as e:
            logger.error(f"Error creating chat engine: {e}")
            raise
    
    def create_folder_chat_engine(self, folder_name: str, similarity_top_k: Optional[int] = None):  # ä¸ºæŒ‡å®šæ–‡ä»¶å¤¹åˆ›å»ºä¸€ä¸ªâ€œä¸“å±èŠå¤©å¼•æ“â€
        """
        Create chat engine for specified folder
        
        Args:
            folder_name: Folder name
            similarity_top_k: Number of similar documents to retrieve
        """
        if folder_name not in self.folder_indexes:
            raise ValueError(f"Index for folder '{folder_name}' does not exist")
        
        if similarity_top_k is None:
            similarity_top_k = RAGConfig.SIMILARITY_TOP_K
        
        try:
            # Create chat memory
            memory = ChatMemoryBuffer.from_defaults(token_limit=RAGConfig.TOKEN_LIMIT)
            
            # Create chat engine
            chat_engine = self.folder_indexes[folder_name].as_chat_engine(
                chat_mode="context",
                memory=memory,
                similarity_top_k=similarity_top_k,
                system_prompt=SystemPrompts.folder_specific(folder_name),
                verbose=True
            )
            
            self.folder_chat_engines[folder_name] = chat_engine
            logger.info(f"Chat engine for folder '{folder_name}' created successfully")
            
        except Exception as e:
            logger.error(f"Error creating chat engine for folder '{folder_name}': {e}")
            raise
    
    def chat_with_folder(self, folder_name: str, message: str) -> str:  # å®é™…å’ŒæŸä¸ªæ–‡ä»¶å¤¹çš„çŸ¥è¯†åº“å¯¹è¯
        """
        Chat with documents in specified folder
        
        Args:
            folder_name: Folder name
            message: User message
            
        Returns:
            AI response
        """
        if folder_name not in self.folder_chat_engines:
            # If chat engine doesn't exist, try to create it
            if folder_name in self.folder_indexes:
                self.create_folder_chat_engine(folder_name)
            else:
                raise ValueError(f"Index for folder '{folder_name}' does not exist")
        
        try:
            response = self.folder_chat_engines[folder_name].chat(message)
            return str(response)
        except Exception as e:
            logger.error(f"Error during folder '{folder_name}' chat: {e}")
            return f"Sorry, an error occurred while processing your question: {e}"
    
    def query_folder(self, folder_name: str, question: str, similarity_top_k: Optional[int] = None) -> str:  # å¯¹æŒ‡å®šæ–‡ä»¶å¤¹çš„çŸ¥è¯†åº“åšä¸€æ¬¡â€œå•è½®æ£€ç´¢å‹é—®ç­”â€
        """
        Query documents in specified folder (single query without conversation history)
        
        Args:å®šä¹‰æ–¹æ³• è¾“å…¥æ˜¯æ–‡ä»¶å¤¹å ç”¨æˆ·æé—® æ£€ç´¢top_kæ•°é‡ è¿”å›å­—ç¬¦ä¸²ç±»å‹ç­”æ¡ˆ
            folder_name: Folder name
            question: Query question
            similarity_top_k: Number of similar documents to retrieve
            
        Returns:
            Query result
        """
        if folder_name not in self.folder_indexes:  # å¦‚æœè¿™ä¸ªæ–‡ä»¶å¤¹è¿˜æ²¡æœ‰æ„å»ºå‘é‡ç´¢å¼• ç›´æ¥æŠ¥é”™
            raise ValueError(f"Index for folder '{folder_name}' does not exist")
        
        if similarity_top_k is None:
            similarity_top_k = RAGConfig.SIMILARITY_TOP_K
        
        try:
            query_engine = self.folder_indexes[folder_name].as_query_engine(  # æŠŠæœ¬æ–‡ä»¶å¤¹çš„å‘é‡ç´¢å¼•å¯¹è±¡è½¬æˆä¸€ä¸ªçº¯æ£€ç´¢çš„query_engine
                similarity_top_k=similarity_top_k,
                response_mode="tree_summarize"  # ç”¨æ ‘çŠ¶æ‘˜è¦æ³• è‡ªåŠ¨æ•´åˆå¤šæ®µæ£€ç´¢ç»“æœ è¾“å‡ºç²¾ç‚¼ç­”æ¡ˆ
            )
            
            response = query_engine.query(question)
            return str(response)
            
        except Exception as e:
            logger.error(f"Error querying folder '{folder_name}': {e}")
            return f"Sorry, an error occurred while processing your query: {e}"
    
    def get_available_folders(self) -> List[str]:
        """
        Get list of available folders
        
        Returns:
            List of folder names
        """
        return list(self.folder_indexes.keys())
    
    def get_folder_documents_count(self, folder_name: str) -> int:
        """
        Get document count for specified folder
        
        Args:
            folder_name: Folder name
            
        Returns:
            Document count
        """
        try:
            folder_path = self.documents_dir / folder_name
            if not folder_path.exists() or not folder_path.is_dir():
                return 0  # å¦‚æœè¯¥è·¯å¾„ä¸å­˜åœ¨ æˆ–è€…ä¸æ˜¯æ–‡ä»¶å¤¹ ç›´æ¥è¿”å›0ï¼ˆæ²¡æœ‰ä»»ä½•æ–‡ä»¶ï¼‰
            
            count = 0
            for file_path in folder_path.rglob("*"):  # ç”¨rglob("*")é€’å½’éå†è¯¥æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰å±‚çº§çš„æ–‡ä»¶å’Œå­æ–‡ä»¶å¤¹
                if file_path.is_file():
                    count += 1  # å¦‚æœéå†åˆ°çš„æ˜¯â€œæ–‡ä»¶â€ï¼Œå°±è®¡æ•°å™¨åŠ 1
            return count
        except Exception as e:
            logger.error(f"Error getting document count for folder '{folder_name}': {e}")
            return 0
    
    def chat(self, message: str) -> str:
        """
        Chat with the system
        
        Args:
            message: User message
            
        Returns:
            System response
        """
        if self.chat_engine is None:
            raise ValueError("Please create chat engine first")
        
        try:
            response = self.chat_engine.chat(message)
            return str(response)
        except Exception as e:
            logger.error(f"Error during chat: {e}")
            return f"Sorry, an error occurred while processing your question: {e}"
    
    def add_document(self, file_path: str) -> bool:
        """
        Add new document to system
        
        Args:
            file_path: File path
            
        Returns:
            Whether successfully added
        """
        try:
            # Copy file to documents directory
            source_path = Path(file_path)  # æŠŠè¾“å…¥çš„æ–‡ä»¶è·¯å¾„ï¼ˆå­—ç¬¦ä¸²ï¼‰è½¬ä¸º Path å¯¹è±¡ã€‚  
            dest_path = self.documents_dir / source_path.name  # æ„é€ ç›®æ ‡è·¯å¾„ï¼ˆç›®æ ‡ç›®å½•æ˜¯ç³»ç»Ÿçš„documentsç›®å½•ï¼Œæ–‡ä»¶åä¿æŒä¸å˜ï¼‰ã€‚
            
            if not dest_path.exists():  # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œé˜²æ­¢é‡å¤æ·»åŠ ã€‚
                shutil.copy2(source_path, dest_path)  # å¦‚æœä¸å­˜åœ¨ï¼Œå¤åˆ¶æºæ–‡ä»¶åˆ°ç³»ç»Ÿæ–‡æ¡£ç›®å½•ã€‚
                logger.info(f"Document {source_path.name} added to system")
                
                # Reload documents and rebuild index
                documents = self.load_documents()  # é‡æ–°æ‰«æå¹¶åŠ è½½æ‰€æœ‰æ–‡æ¡£ï¼Œè·å–æ–°çš„æ–‡æ¡£å¯¹è±¡åˆ—è¡¨ã€‚
                if documents:  # å¦‚æœæ–‡æ¡£åˆ—è¡¨ä¸ä¸ºç©ºï¼Œå¼ºåˆ¶é‡å»ºå‘é‡ç´¢å¼•ï¼Œä¿è¯æ–°æ–‡ä»¶è¢«çº³å…¥çŸ¥è¯†åº“ã€‚
                    self.build_index(documents, force_rebuild=True)
                    if self.chat_engine:
                        self.create_chat_engine()  # å¦‚æœèŠå¤©å¼•æ“å·²ç»å­˜åœ¨ï¼Œåˆ™é‡æ–°åˆ›å»ºï¼Œç¡®ä¿å’Œæ–°çŸ¥è¯†åº“åŒæ­¥
                    return True
            else:
                logger.warning(f"Document {source_path.name} already exists")
                return False
                
        except Exception as e:
            logger.error(f"Error adding document: {e}")
            return False
    
    def list_documents(self) -> List[str]:  # å®Œå…¨æ”¯æŒæœ‰å¤šå±‚å­æ–‡ä»¶å¤¹çš„ç›®å½•ç»“æ„
        """
        List all documents in system (recursively scan subdirectories)
        
        Returns:
            List of document filenames (including relative paths)
        """
        try:
            files = []
            # Recursively scan files in all subdirectories
            for file_path in self.documents_dir.rglob("*"):  # ä¼šé€’å½’éå†documents_dirä¸‹çš„æ‰€æœ‰å†…å®¹ï¼ŒåŒ…æ‹¬æ‰€æœ‰å­æ–‡ä»¶å¤¹é‡Œçš„æ–‡ä»¶ï¼Œä¸ç®¡æœ‰å‡ å±‚ã€‚
                if file_path.is_file():
                    # Get path relative to documents directory
                    relative_path = file_path.relative_to(self.documents_dir)  # ä¼šæŠŠæ¯ä¸ªæ–‡ä»¶çš„å…¨è·¯å¾„è½¬æ¢æˆç›¸å¯¹äºæ ¹ç›®å½•çš„è·¯å¾„ï¼Œæ¯”å¦‚ï¼š
                    files.append(str(relative_path))
            return sorted(files)  # æœ€ç»ˆæ‰€æœ‰æ–‡ä»¶ï¼ˆæ— è®ºåœ¨å“ªä¸€å±‚ï¼‰éƒ½ä¼šè¢«åŒ…å«è¿›è¿”å›ç»“æœï¼Œè€Œä¸”æ˜¯æ¸…æ¥šçš„â€œå¸¦å±‚æ¬¡â€çš„ç›¸å¯¹è·¯å¾„ã€‚
        except Exception as e:
            logger.error(f"Error listing documents: {e}")
            return []
        