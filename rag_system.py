import time
import torch
import random
import logging
import hashlib
import numpy as np
from tqdm import tqdm
from pathlib import Path
from typing import List, Optional, Dict

from llama_index.core import (
    VectorStoreIndex,  # åˆ›å»ºå‘é‡ç´¢å¼•å¯¹è±¡
    SimpleDirectoryReader,
    Document,
    StorageContext,  # ç´¢å¼•æ•°æ®è¦å­˜åœ¨å“ªé‡Œ æ€ä¹ˆå­˜ ç”¨ä»€ä¹ˆæ–¹å¼å­˜
    load_index_from_storage,  # ç”¨äºä»æœ¬åœ°åŠ è½½å·²æœ‰çš„ç´¢å¼• è¦ä¼ å…¥ä¸€ä¸ªStorageContextå¯¹è±¡
    Settings
)

from llama_index.core.node_parser import SentenceSplitter  # node_parserç”¨åˆ°äº†
from llama_index.llms.openai_like import OpenAILike
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.memory import ChatMemoryBuffer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGConfig:
    # Document processing configuration
    CHUNK_SIZE = 512
    CHUNK_OVERLAP = 50
    SUPPORTED_EXTENSIONS = [".pdf", ".docx", ".doc", ".txt", ".md", ".html"]
    
    # LLM configuration
    DEFAULT_TEMPERATURE = 0.7
    DEFAULT_MAX_TOKENS = 2048  # é™åˆ¶LLMç”Ÿæˆçš„æœ€å¤§tokenæ•°
    
    # Retrieval configuration
    SIMILARITY_TOP_K = 3
    TOKEN_LIMIT = 3000  # å¤šè½®å¯¹è¯æ—¶ ä¿å­˜å¤šå°‘tokençš„å†å²
    
    # Random seed
    RANDOM_SEED = 42


class SystemPrompts:
    DEFAULT = (
        "You are an intelligent assistant that answers user questions based on provided document content."
        "Please answer based on the retrieved relevant document content. If there is no relevant information in the documents, please clearly state so."
        "When answering, please be accurate, detailed, and helpful. If possible, please cite specific document sources."
    )
# Answer ONLY with the facts listed in the sources below.
# If there isnâ€™t enough information below, say you donâ€™t know. DO NOT generate answers that donâ€™t use the sources below. 
# If asking a clarifying question to the user would help, ask the question.
# If the question is not in English, translate the question to English before generating the search query.

    @staticmethod
    def folder_specific(folder_name: str) -> str:
        return (
            f"You are an intelligent assistant that specializes in answering user questions based on document content from the '{folder_name}' folder."
            "Please answer based on the retrieved relevant document content. If there is no relevant information in the documents, please clearly state so."
            "When answering, please be accurate, detailed, and helpful. If possible, please cite specific document sources."
        )


class MultiModelRAGSystem:
    def __init__(self, api_key: str, model_type: str = "deepseek-chat", documents_dir: str = "./documents", storage_dir: str = "./storage", embedding_model: str = "BAAI/bge-small-zh-v1.5", random_seed: Optional[int] = None):
        # Set random seed for reproducibility
        self.random_seed = random_seed or RAGConfig.RANDOM_SEED  # å¦‚æœinitæ²¡ä¼  æœ‰é»˜è®¤çš„
        self._set_random_seeds()  # defined function below
        
        self.api_key = api_key
        self.model_type = model_type.lower()
        self.documents_dir = Path(documents_dir)
        self.storage_dir = Path(storage_dir)
        
        # Create directories
        self.documents_dir.mkdir(exist_ok=True)
        self.storage_dir.mkdir(exist_ok=True)
        
        # Configure LLM based on model type
        self.llm = self._configure_llm()  # åˆå§‹åŒ–LLMå¯¹è±¡ # defined function below
        
        logger.info(f"Using the LLM API: {self.model_type}.")
        
        # Configure embedding model (using local model with CUDA acceleration)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Detected Device in Current Environment: {device}.")

        self.embed_model = HuggingFaceEmbedding(
            model_name=embedding_model,
            cache_folder="./embedding_cache",
            device=device
        )
        
        # Configure global settings
        Settings.llm = self.llm
        Settings.embed_model = self.embed_model
        Settings.chunk_size = RAGConfig.CHUNK_SIZE
        Settings.chunk_overlap = RAGConfig.CHUNK_OVERLAP
        
        # Configure node parser
        self.node_parser = SentenceSplitter(
            chunk_size=RAGConfig.CHUNK_SIZE,
            chunk_overlap=RAGConfig.CHUNK_OVERLAP,
        )
        
        self.index = None
        self.chat_engine = None
        
        # Folder-based indexing functionality
        self.folder_indexes: Dict[str, VectorStoreIndex] = {}
        self.folder_chat_engines: Dict[str, any] = {}
    
    def _set_random_seeds(self) -> None:
        """Set random seeds for reproducibility"""
        random.seed(self.random_seed)
        np.random.seed(self.random_seed)
        torch.manual_seed(self.random_seed)  # PyTorchçš„CPUéšæœºæ•°ç”Ÿæˆå™¨
        if torch.cuda.is_available():  # GPU
            torch.cuda.manual_seed(self.random_seed)
            torch.cuda.manual_seed_all(self.random_seed)
        # Set deterministic behavior
        # è¿™ä¸¤è¡Œç”¨äºå½»åº•ä¿è¯æ“ä½œçš„ç¡®å®šæ€§ æœ‰çš„åº•å±‚å®ç°æœ‰â€œéç¡®å®šæ€§åŠ é€Ÿä¼˜åŒ–â€ è¿™æ ·å†™èƒ½å…³æ‰æ‰€æœ‰ä¸ç¡®å®šæ€§æ“ä½œ ç‰ºç‰²ä¸€ç‚¹é€Ÿåº¦æ¢æ¥â€œå®Œå…¨å¯å¤ç°â€
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        logger.info(f"Manually Set Random Seed: {self.random_seed} in All Libraries.")
    
    def _get_base_llm_config(self) -> dict:
        """Get base LLM configuration"""
        # ä¸“é—¨è´Ÿè´£åŸºç¡€å…¬å…±é…ç½® api_key temperature max_tokens è¿™äº›å‚æ•°éƒ½é€šç”¨
        return {
            'api_key': self.api_key,
            'temperature': RAGConfig.DEFAULT_TEMPERATURE,
            'max_tokens': RAGConfig.DEFAULT_MAX_TOKENS,
            'is_chat_model': True,  # å£°æ˜è¿™æ˜¯â€œå¯¹è¯å¤§æ¨¡å‹â€
        }
    
    def _configure_llm(self):  # æ ¹æ®é€‰çš„æ¨¡å‹ç±»å‹ åŠ¨æ€åˆæˆå…¨é‡é…ç½® å¹¶åˆ›å»ºæ¨¡å‹å¯¹è±¡
        """
        Configure LLM based on model type
        
        Returns:
            Configured LLM instance
        """
        base_config = self._get_base_llm_config()
        
        model_configs = {
            "deepseek-chat": {
                "model": "deepseek-chat",  # DeepSeek-V3-0324 Jul-22
                "api_base": "https://api.deepseek.com/v1",  
            },
            "deepseek-reasoner": {
                "model": "deepseek-reasoner",  # DeepSeek-R1-0528 JUl-22
                "api_base": "https://api.deepseek.com/v1",
            },
            "gpt4.1": {
                "model": "gpt-4.1-2025-04-14",
                "api_base": "https://api.openai.com/v1",
            },
            "gpt4o": {
                "model": "gpt-4o-2024-08-06",
                "api_base": "https://api.openai.com/v1",
            }
        }
        
        if self.model_type not in model_configs:
            raise ValueError(f"Unsupported LLM API: {self.model_type}. Supported Ones: {', '.join(model_configs.keys())}")
        # base_config æ˜¯é€šç”¨çš„LLMé…ç½®ä¿¡æ¯;
        # model_configs æ˜¯å…·ä½“LLMå“ç‰Œçš„é…ç½®ä¿¡æ¯
        config = {**base_config, **model_configs[self.model_type]}
        return OpenAILike(**config)  # å‚æ•°è§£åŒ… Keyword Arguments unpacking
        
    def _add_document_metadata(self, doc: Document, folder_name: Optional[str] = None) -> None:
        """Add standard metadata to document"""
        # hasattr æ˜¯å¦æœ‰æŸä¸ªå±æ€§
        if hasattr(doc, 'metadata') and 'file_path' in doc.metadata:
            file_path = Path(doc.metadata['file_path'])
            doc.metadata.update({
                'filename': file_path.name,
                'file_type': file_path.suffix,
                'file_size': file_path.stat().st_size if file_path.exists() else 0
            })
            if folder_name:
                doc.metadata['folder'] = folder_name
                
    def load_documents(self, file_extensions: Optional[List[str]] = None) -> List[Document]:
        """
        Load documents from specified directory
        
        Args:
            file_extensions: List of supported file extensions
            
        Returns:
            List of documents
        """
        if file_extensions is None:
            file_extensions = RAGConfig.SUPPORTED_EXTENSIONS
        
        try:
            reader = SimpleDirectoryReader(
                input_dir=str(self.documents_dir),
                recursive=True,
                required_exts=file_extensions,
            )
            
            documents = reader.load_data(show_progress=True,num_workers=4)
            logger.info(f"Successfully Loaded {len(documents)} Document Pages.")  # å¤šå°‘page
            
            # Add metadata to each document
            for doc in documents:
                self._add_document_metadata(doc)  # ç»™æ¯ä¸€ä¸ªdocæ·»åŠ å…ƒæ•°æ®
            ########################################################################################
            unique_docs = []
            seen_hashes = set()
            for doc in documents:
                content_hash = hashlib.md5(doc.text.encode('utf-8')).hexdigest()  # å¯¹æ¯ä¸ªé¡µé¢å†…å®¹ï¼ˆdoc.textï¼‰åšmd5å“ˆå¸Œè¿ç®—ï¼Œå¾—åˆ°ä¸€ä¸ªå”¯ä¸€çš„â€œæŒ‡çº¹â€ã€‚
                if content_hash not in seen_hashes:
                    seen_hashes.add(content_hash)
                    unique_docs.append(doc)
            logger.info(f"There Were {len(documents)} Document Pages Before.")
            logger.info(f"After Hash Deduplication, {len(unique_docs)} Document Pages Remain.")
            return unique_docs
            #########################################################################################
            # return documents
            
        except Exception as e:
            logger.error(f"Error on Loading Document Page: {e}.")
            return []
    
    def build_index(self, documents: List[Document], force_rebuild: bool = False) -> None:
        """
        Build or load vector index
        
        Args:
            documents: List of documents
            force_rebuild: Whether to force rebuild index
        """
        index_store_file = self.storage_dir / "index_store.json"
        docstore_file = self.storage_dir / "docstore.json"
        
        try:
            # Check if saved index exists
            if index_store_file.exists() and docstore_file.exists() and not force_rebuild:
                start_time = time.time()
                logger.info("Loading Existing Index for Global Chat Engine...")

                # StorageContext,  # ç´¢å¼•æ•°æ®è¦å­˜åœ¨å“ªé‡Œ æ€ä¹ˆå­˜ ç”¨ä»€ä¹ˆæ–¹å¼å­˜
                # load_index_from_storage,  # ç”¨äºä»æœ¬åœ°åŠ è½½å·²æœ‰çš„ç´¢å¼• è¦ä¼ å…¥ä¸€ä¸ªStorageContextå¯¹è±¡

                with tqdm(total=1, desc="Loading Existing Index for Global Chat Engine.", unit="Step") as pbar:
                    storage_context = StorageContext.from_defaults(persist_dir=str(self.storage_dir))  # åˆ›å»ºä¸€ä¸ªç´¢å¼•å­˜å‚¨çš„ä¸Šä¸‹æ–‡å¯¹è±¡ ç”¨äºåç»­ç´¢å¼•çš„åŠ è½½ è¿™æ˜¯llama_indexè¯»å–æœ¬åœ°å‘é‡ç´¢å¼•æ—¶çš„æ ‡å‡†å†™æ³•
                    self.index = load_index_from_storage(storage_context)  # ç”¨llama_indexçš„æ¥å£ æŠŠæœ¬åœ°å­˜å‚¨é‡Œçš„å‘é‡ç´¢å¼•æ•°æ®åŠ è½½åˆ°å†…å­˜ å¹¶èµ‹å€¼ç»™self.index åç»­å°±å¯ä»¥æ£€ç´¢äº†
                    pbar.update(1)  # æ‰‹åŠ¨æŠŠè¿›åº¦æ¡æ¨è¿›1æ­¥ è¡¨ç¤ºâ€œåŠ è½½ç´¢å¼•â€è¿™ä¸€æ­¥å·²ç»å®Œæˆ
                load_time = time.time() - start_time
                logger.info(f"Loaded Existing Index for Global Chat Engine in {load_time:.2f} Seconds.")
            else:
                # If no documents provided and no existing index, skip building
                if not documents:  # å¦‚æœdocumentsè¿™ä¸ªåˆ—è¡¨æœ¬èº«æ˜¯ç©ºçš„ï¼ˆæ²¡æœ‰å¯ç”¨æ–‡æ¡£ï¼‰é‚£è¿ç´¢å¼•éƒ½æ²¡æ³•å»º
                    logger.info("No Document Found and No Existing Index Found.")
                    self.index = None
                    return
                
                start_time = time.time()  ####################
                logger.info("Building New Index...")

                logger.info("Firstly, Parse Document Page.")
                # Parse document nodes åˆ‡å—æ“ä½œ
                parse_start = time.time()  ####################
                with tqdm(total=len(documents), desc="Parsing Document Pages.", unit="Document") as pbar:  # æ€»æ•°=æ–‡æ¡£æ•° å•ä½æ˜¯doc
                    nodes = []
                    for doc in documents:  # éå†æ‰€æœ‰æ–‡æ¡£  æ‰€æœ‰åˆ‡çš„å—
                        doc_nodes = self.node_parser.get_nodes_from_documents([doc])  # æ¯ä»½æ–‡æ¡£ç”¨node_parserï¼ˆé€šå¸¸æ˜¯åˆ†å¥/åˆ†å—å™¨ï¼‰åˆ‡åˆ†æˆè‹¥å¹²â€œèŠ‚ç‚¹â€ï¼ˆchunkï¼‰
                        nodes.extend(doc_nodes)  # æŠŠè¿™äº›èŠ‚ç‚¹åˆå¹¶åˆ°nodesæ€»åˆ—è¡¨
                        pbar.update(1)  # bar+=1
                parse_time = time.time() - parse_start  #################### åˆ‡å—æ—¶é—´
                
                logger.info(f"Document Pages Split Into {len(nodes)} Nodes in {parse_time:.2f} Seconds.")  # è¿™æ¬¡æ‰€æœ‰æ–‡æ¡£æ€»å…±åˆ‡æˆå¤šå°‘ä¸ªèŠ‚ç‚¹ æ¯ä¸ªèŠ‚ç‚¹åç»­éƒ½ä¼šåšembedding

                # Create vector index åšembedding ç”Ÿæˆå‘é‡å¹¶å»ºç«‹ç´¢å¼•
                logger.info("Secondly, Create Index.")
                index_start = time.time()  ####################
                with tqdm(total=1, desc="Creating Vector Index.", unit="Step") as pbar:
                    self.index = VectorStoreIndex(nodes, show_progress=True)  # è°ƒç”¨VectorStoreIndex(nodes, ...) å¯¹æ‰€æœ‰èŠ‚ç‚¹åšembeddingå¹¶å»ºç«‹å‘é‡æ£€ç´¢ç´¢å¼• ç´¢å¼•å¯¹è±¡èµ‹å€¼ç»™self.index ä¾›åç»­æ£€ç´¢ç”¨
                    pbar.update(1)  # æ‰‹åŠ¨+1
                index_time = time.time() - index_start  ####################ç”Ÿæˆå‘é‡å¹¶å»ºç«‹ç´¢å¼•ç”¨äº†å¤šé•¿æ—¶é—´
                
                # Save index ä¿å­˜åˆ°æœ¬åœ°ç£ç›˜
                logger.info("Thirdly, Save Index.")
                save_start = time.time()  ####################
                with tqdm(total=1, desc="Saving Index.", unit="Step") as pbar:
                    self.index.storage_context.persist(persist_dir=str(self.storage_dir))  # æŠŠå½“å‰ç´¢å¼•å®Œæ•´ä¿å­˜åˆ°æœ¬åœ°æŒ‡å®šç›®å½•
                    pbar.update(1)
                save_time = time.time() - save_start  ####################ä¿å­˜åˆ°æœ¬åœ°ç£ç›˜ç”¨äº†å¤šé•¿æ—¶é—´
                
                total_time = time.time() - start_time  #################### ä¸€å…±èŠ±äº†å¤šé•¿æ—¶é—´
                logger.info(f"Build Index Successfully.")
                logger.info(f"Document Page Parsed, Vector Index Built, and Saved Successfully in {total_time:.2f} Seconds Totally.")
                logger.info(f"Parsing in {parse_time:.2f} Seconds,   Building in {index_time:.2f} Seconds,   Saving in {save_time:.2f} Seconds.")
                
        except Exception as e:
            logger.error(f"Error on Building Index: {e}.")
            raise
    
    def build_folder_indexes(self, force_rebuild: bool = False) -> None:  # åˆ†æ–‡ä»¶å¤¹æ‰¹é‡æ„å»ºç´¢å¼•çš„æ ¸å¿ƒå‡½æ•°
        """
        Build indexes grouped by folder æ‰¹é‡ä¸ºæ¯ä¸ªæ–‡ä»¶å¤¹ä¸‹çš„æ–‡æ¡£æ„å»ºå‘é‡ç´¢å¼•
        
        Args:
            force_rebuild: Whether to force rebuild indexes
        """
        try:
            overall_start_time = time.time()  # æ•´ä¸ªè¿‡ç¨‹çš„èµ·å§‹æ—¶é—´
            # Get all subfolders
            folders = []
            for item in self.documents_dir.iterdir():  # è·å–æ–‡æ¡£ä¸»ç›®å½•ä¸‹çš„æ‰€æœ‰å­æ–‡ä»¶å¤¹åç§° iterdir(): åˆ—ä¸¾å½“å‰ç›®å½•ä¸‹æ‰€æœ‰çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹
                if item.is_dir():
                    folders.append(item.name)
            
            logger.info(f"Found {len(folders)} Folders: {folders}.")
            logger.info("ğŸ“Š START RECORD TIME ABOUT FOLDER INDEX.")
            
            # Statistics tracking
            folder_stats = []
            total_documents = 0
            total_nodes = 0
            successful_builds = 0
            
            # Iterate through folders with progress bar
            for folder_name in tqdm(folders, desc="Processing Folders.", unit="Folder"):
                folder_start_time = time.time()  # è®°å½•æ¯ä¸ªæ–‡ä»¶å¤¹çš„èµ·å§‹æ—¶é—´folder_start_time
                folder_path = self.documents_dir / folder_name
                storage_path = self.storage_dir / f"index_{folder_name}"
                
                logger.info(f"ğŸ“ Processing Folder: '{folder_name}'.")
                
                # Check if saved index exists
                if storage_path.exists() and not force_rebuild:
                    load_start = time.time()  ### è®°å½•åŠ è½½ç°æœ‰ç´¢å¼•çš„å¼€å§‹æ—¶é—´
                    logger.info(f"Loading Existing Folder Index for Folder '{folder_name}'...")
                    try:  # å°è¯•åŠ è½½æŒ‡å®šæ–‡ä»¶å¤¹å·²æœ‰çš„å‘é‡ç´¢å¼•ï¼ˆå¦‚æœå·²ç»æ„å»ºè¿‡ï¼‰å¹¶æŠŠç»“æœä¿å­˜åˆ°ç³»ç»Ÿå†…å­˜çš„ç´¢å¼•å­—å…¸é‡Œ
                        storage_context = StorageContext.from_defaults(persist_dir=str(storage_path))  # åˆ›å»ºä¸€ä¸ªç´¢å¼•å­˜å‚¨çš„ä¸Šä¸‹æ–‡å¯¹è±¡ ç”¨äºåç»­ç´¢å¼•çš„åŠ è½½ è¿™æ˜¯llama_indexè¯»å–æœ¬åœ°å‘é‡ç´¢å¼•æ—¶çš„æ ‡å‡†å†™æ³•
                        self.folder_indexes[folder_name] = load_index_from_storage(storage_context)  # æŠŠç£ç›˜ä¸Šçš„å‘é‡ç´¢å¼•æ•°æ®åŠ è½½åˆ°å†…å­˜ å¹¶æ”¾è¿› self.folder_indexes è¿™ä¸ªå­—å…¸é‡Œï¼ˆä»¥æ–‡ä»¶å¤¹åä¸ºkeyï¼‰
                        load_time = time.time() - load_start  ### åŠ è½½ç°æœ‰ç´¢å¼•ç”¨äº†å¤šå°‘æ—¶é—´
                        folder_total_time = time.time() - folder_start_time  ### æ•´ä¸ªè¿™ä¸ªæ–‡ä»¶å¤¹çš„ç”¨æ—¶
                        
                        logger.info(f"Loaded Existing Folder Index for Folder '{folder_name}' Successfully.")
                        logger.info(f"Load Time: {load_time:.2f} Seconds,   Total Time: {folder_total_time:.2f} Seconds.")
                        
                        folder_stats.append({
                            'name': folder_name,
                            'action': 'loaded',
                            'total_time': folder_total_time,
                            'load_time': load_time,
                            'documents': 0,
                            'nodes': 0
                        })
                        successful_builds += 1
                        continue  # è·³è¿‡æœ¬è½®å¾ªç¯ å·²ç»åŠ è½½å®Œæˆ ä¸éœ€è¦ç»§ç»­å¤„ç† å°±æ˜¯åé¢çš„ä»£ç 
                    except Exception as e:
                        logger.error(f"Failed to Load Existing Folder Index '{folder_name}', {e}.")
                
                # Load documents from this folder
                try:
                    logger.info(f"ğŸš€ Building Folder Index for Folder '{folder_name}'...")
                    doc_load_start = time.time()  ### è®°å½•æ–‡æ¡£åŠ è½½çš„èµ·å§‹æ—¶é—´
                    reader = SimpleDirectoryReader(
                        input_dir=str(folder_path),
                        recursive=True,
                        required_exts=RAGConfig.SUPPORTED_EXTENSIONS,
                    )
                    
                    documents = reader.load_data()
                    doc_load_time = time.time() - doc_load_start  ### è®°å½•æ–‡æ¡£åŠ è½½èŠ±äº†å¤šé•¿æ—¶é—´
                    
                    if not documents:  # å¦‚æœæ²¡è¯»åˆ°æ–‡æ¡£ åˆ—è¡¨ä¸ºç©º è¾“å‡ºè­¦å‘Š
                        logger.error(f"âš ï¸ No Document Found in Folder '{folder_name}'.")
                        folder_total_time = time.time() - folder_start_time  ###
                        folder_stats.append({
                            'name': folder_name,
                            'action': 'skipped',
                            'total_time': folder_total_time,
                            'documents': 0,
                            'nodes': 0
                        })
                        continue  # è·³è¿‡è¿™æ¬¡å¾ªç¯
                    
                    # Add folder identification and metadata to documents
                    for doc in documents:
                        self._add_document_metadata(doc, folder_name)  # ç»™è¿™äº›å­æ–‡ä»¶å¤¹çš„æ–‡ä»¶æ·»åŠ metadata å®šä¹‰å¥½çš„å‡½æ•°
                    
                    logger.info(f"Read {len(documents)} Document Pages in {doc_load_time:.2f} Seconds under Current Folder '{folder_name}'.")

                    # Parse document nodes
                    logger.info("Firstly, Parse Document Pages under Current Folder.")
                    parse_start = time.time()  ### è®°å½•â€œæ‹†åˆ†èŠ‚ç‚¹â€å¼€å§‹æ—¶é—´
                    with tqdm(total=len(documents), desc=f"Parsing Document Page under Folder: {folder_name}.", unit="Page", leave=False) as pbar:  # False è¿›åº¦æ¡ç”¨å®Œåè¦ä¸è¦ç•™åœ¨ç»ˆç«¯ï¼ˆæˆ–æ—¥å¿—ï¼‰é‡Œã€‚
                        nodes = []
                        for doc in documents:
                            doc_nodes = self.node_parser.get_nodes_from_documents([doc])
                            nodes.extend(doc_nodes)
                            pbar.update(1)  # è¿›åº¦æ¡å‰è¿›1æ­¥
                    parse_time = time.time() - parse_start  ### æ‹†åˆ†è¿™äº›èŠ‚ç‚¹æ‰€ç”¨æ—¶é—´
                    
                    logger.info(f"Document Pages Split Into {len(nodes)} Nodes in {parse_time:.2f} Seconds under Current Folder '{folder_name}'.")
                    
                    # Create vector index åˆ›å»ºembeddingå‘é‡ç´¢å¼•
                    logger.info("Secondly, Create Folder Index under Current Folder.")
                    index_start = time.time()
                    with tqdm(total=1, desc=f"Creating Folder Index for Folder {folder_name}.", unit="Step", leave=False) as pbar:
                        folder_index = VectorStoreIndex(nodes, show_progress=True)
                        pbar.update(1)
                    index_time = time.time() - index_start  # è®°å½•ç´¢å¼•æ„å»ºæ‰€ç”¨çš„æ—¶é—´
                    
                    # Save index
                    logger.info("Thirdly, Save Folder Index under Current Folder.")
                    save_start = time.time()
                    with tqdm(total=1, desc=f"Saving Folder Index for Folder {folder_name}.", unit="Step", leave=False) as pbar:
                        storage_path.mkdir(exist_ok=True)
                        folder_index.storage_context.persist(persist_dir=str(storage_path))  # æŠŠåˆšåˆšå»ºç«‹çš„ç´¢å¼•ç»“æ„ å‘é‡ å…ƒæ•°æ®ç­‰ ä¿å­˜åˆ°ç£ç›˜ä¸Š æ–¹ä¾¿ä¸‹æ¬¡ç›´æ¥åŠ è½½ æ— éœ€é‡å»º
                        pbar.update(1)
                    save_time = time.time() - save_start  # ä¿å­˜ç´¢å¼•è€—æ—¶
                    
                    # Store in memory
                    self.folder_indexes[folder_name] = folder_index  # åœ¨é‚£ä¸ªå­—å…¸ç»“æ„é‡Œä¿å­˜ä¸€ä¸‹
                    
                    folder_total_time = time.time() - folder_start_time  # ç»Ÿè®¡æœ¬æ¬¡æ€»ç”¨æ—¶ï¼ˆä»è¿™ä¸ªå­æ–‡ä»¶å¤¹å¼€å§‹å¤„ç†åˆ°ç°åœ¨ï¼‰

                    logger.info(f"Already Build Folder Index for Folder '{folder_name}' Successfully.")
                    logger.info(f"Total Time: {folder_total_time:.2f} Seconds,   Read Time: {doc_load_time:.2f} Seconds,   Parsed Time: {parse_time:.2f} Seconds,   Built Time: {index_time:.2f} Seconds,   Saved Time: {save_time:.2f} Seconds.")
                    
                    # Update statistics
                    total_documents += len(documents)
                    total_nodes += len(nodes)
                    successful_builds += 1
                    
                    folder_stats.append({
                        'name': folder_name,
                        'action': 'built',
                        'total_time': folder_total_time,
                        'doc_load_time': doc_load_time,
                        'parse_time': parse_time,
                        'index_time': index_time,
                        'save_time': save_time,
                        'documents': len(documents),
                        'nodes': len(nodes)
                    })
                    
                except Exception as e:
                    folder_total_time = time.time() - folder_start_time
                    logger.error(f"Error on Building Folder Index for Folder '{folder_name}': {e} (Time: {folder_total_time:.2f} Seconds).")
                    folder_stats.append({
                        'name': folder_name,
                        'action': 'failed',
                        'total_time': folder_total_time,
                        'error': str(e)
                    })
                    continue  # continue è·³è¿‡æœ¬è½® è¿›å…¥ä¸‹ä¸€ä¸ªæ–‡ä»¶å¤¹
            
            overall_time = time.time() - overall_start_time
            
            # Print detailed timing report
            logger.info("DETAILED TIMING REPORT FOR FOLDER INDEX.")
            # Sort by total time for better analysis
            folder_stats.sort(key=lambda x: x.get("total_time", 0), reverse=True)
            for stat in folder_stats:
                if stat['action'] == 'built':
                    logger.info(f"ğŸ“ Folder {stat['name']:20}, TOTAL BUILT TIME: {stat['total_time']:6.2f}s, PAGE COUNT: {stat['documents']:3d}, NODE COUNT: {stat['nodes']:5d}, READ TIME: {stat['doc_load_time']:4.2f}s, PARSE TIME: {stat['parse_time']:4.2f}s, CREATE INDEX TIME: {stat['index_time']:4.2f}s, SAVE TIME: {stat['save_time']:4.2f}s.")

                elif stat['action'] == 'failed':  # æ„å»ºå¤±è´¥ æ‰“å°é”™è¯¯å†…å®¹
                    logger.info(f"âŒ FAILED on Folder {stat['name']:20}, Running Time: {stat['total_time']:6.2f} Seconds, With Error: {stat.get('error', 'Unknown Error')}.")
                elif stat['action'] == 'skipped':  # æ–‡ä»¶å¤¹æ²¡æœ‰æ–‡æ¡£ ç›´æ¥è·³è¿‡ è¾“å‡ºæç¤º
                    logger.info(f"â­ï¸ SKIPPED   Folder {stat['name']:20}, Running Time: {stat['total_time']:6.2f} Seconds, No Document Found.")
            
            # æ€»ç”¨æ—¶ æ–‡ä»¶å¤¹æ•° æˆåŠŸæ„å»º/åŠ è½½æ•° æ–‡æ¡£æ€»æ•° èŠ‚ç‚¹æ€»æ•° å†…å­˜é‡Œæœ€ç»ˆçš„ç´¢å¼•æ•°é‡
            logger.info("ğŸ“Š STATISTICS SUMMARY FOR FOLDER INDEX.")
            logger.info(f"âœ… Total Time Taken:                {overall_time:.2f} Seconds.")
            logger.info(f"âœ… Total Folder Processed:          {len(folders)}.")
            logger.info(f"âœ… Total Folder Index Built/Loaded: {successful_builds}.")
            logger.info(f"âœ… Total Folder Index:              {len(self.folder_indexes)}.")
            logger.info(f"âœ… Total Document Page Processed:   {total_documents}.")
            logger.info(f"âœ… Total Node Created:              {total_nodes}.")
            
            
        except Exception as e:
            logger.error(f"Error on Building Folder Index: {e}.")
            raise
    
    def create_chat_engine(self, similarity_top_k: Optional[int] = None):
        """
        Create chat engine
        
        Args:
            similarity_top_k: Number of similar documents to retrieve
        """
        if self.index is None:
            raise ValueError("Please Build Global Chat Engine Index First.")
        
        if similarity_top_k is None:
            similarity_top_k = RAGConfig.SIMILARITY_TOP_K
        
        try:
            # Create chat memory
            memory = ChatMemoryBuffer.from_defaults(token_limit=RAGConfig.TOKEN_LIMIT)
            
            # Create chat engine
            self.chat_engine = self.index.as_chat_engine(
                chat_mode="context",  # ç”¨ä¸Šä¸‹æ–‡æ¨¡å¼ æ¯æ¬¡å¯¹è¯éƒ½ç»“åˆå†å²å’Œæ£€ç´¢å†…å®¹
                memory=memory,
                similarity_top_k=similarity_top_k,
                system_prompt=SystemPrompts.DEFAULT,  # æ ‡å‡†æç¤ºè¯ ä¸æ˜¯æ¨¡ç‰ˆçš„é‚£ä¸ª
                verbose=True #  è¯¦ç»†è¾“å‡ºå†…éƒ¨æ—¥å¿— ä¾¿äºè°ƒè¯•
            )
            logger.info(f"Global Chat Engine Created Successfully with Maximal {RAGConfig.TOKEN_LIMIT} MEMORY Tokens.")
            
        except Exception as e:
            logger.error(f"Error on Creating Global Chat Engine: {e}.")
            raise
    
    def create_folder_chat_engine(self, folder_name: str, similarity_top_k: Optional[int] = None):  # ä¸ºæŒ‡å®šæ–‡ä»¶å¤¹åˆ›å»ºä¸€ä¸ªâ€œä¸“å±èŠå¤©å¼•æ“â€
        """
        Create chat engine for specified folder
        
        Args:
            folder_name: Folder name
            similarity_top_k: Number of similar documents to retrieve
        """
        if folder_name not in self.folder_indexes:
            raise ValueError(f"Folder Index for Folder '{folder_name}' Not Found.")
        
        if similarity_top_k is None:
            similarity_top_k = RAGConfig.SIMILARITY_TOP_K
        
        try:
            # Create chat memory
            memory = ChatMemoryBuffer.from_defaults(token_limit=RAGConfig.TOKEN_LIMIT)
            
            # Create chat engine
            chat_engine = self.folder_indexes[folder_name].as_chat_engine(
                chat_mode="context",
                memory=memory,
                similarity_top_k=similarity_top_k,
                system_prompt=SystemPrompts.folder_specific(folder_name),
                verbose=True
            )

            self.folder_chat_engines[folder_name] = chat_engine
            logger.info(f"ğŸš€ Expert Chat Engine for Folder '{folder_name}' Created Successfully.")
            
        except Exception as e:
            logger.error(f"Error on Creating Expert Chat Engine for Folder '{folder_name}'; {e}.")
            raise
    
    def chat_with_folder(self, folder_name: str, message: str) -> str:  # å®é™…å’ŒæŸä¸ªæ–‡ä»¶å¤¹çš„çŸ¥è¯†åº“å¯¹è¯
        """
        Chat with documents in specified folder
        
        Args:
            folder_name: Folder name
            message: User message
            
        Returns:
            AI response
        """
        if folder_name not in self.folder_chat_engines:
            # If chat engine doesn't exist, try to create it
            if folder_name in self.folder_indexes:
                self.create_folder_chat_engine(folder_name)
            else:
                raise ValueError(f"Folder Index for Folder '{folder_name}' Not Found.")
        
        try:
            response = self.folder_chat_engines[folder_name].chat(message)
            return str(response)
        except Exception as e:
            logger.error(f"Error During Chat under Folder '{folder_name}'; {e}.")
            return f"Sorry, Error Occurred When Processing the Question; {e}."
    
    def get_available_folders(self) -> List[str]:
        """
        Get list of available folders
        
        Returns:
            List of folder names
        """
        return list(self.folder_indexes.keys())
    
    def get_folder_documents_count(self, folder_name: str) -> int:
        """
        Get document count for specified folder
        
        Args:
            folder_name: Folder name
            
        Returns:
            Document count
        """
        try:
            folder_path = self.documents_dir / folder_name
            if not folder_path.exists() or not folder_path.is_dir():
                return 0  # å¦‚æœè¯¥è·¯å¾„ä¸å­˜åœ¨ æˆ–è€…ä¸æ˜¯æ–‡ä»¶å¤¹ ç›´æ¥è¿”å›0ï¼ˆæ²¡æœ‰ä»»ä½•æ–‡ä»¶ï¼‰
            
            count = 0
            for file_path in folder_path.rglob("*"):  # ç”¨rglob("*")é€’å½’éå†è¯¥æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰å±‚çº§çš„æ–‡ä»¶å’Œå­æ–‡ä»¶å¤¹
                if file_path.is_file():
                    count += 1  # å¦‚æœéå†åˆ°çš„æ˜¯â€œæ–‡ä»¶â€ï¼Œå°±è®¡æ•°å™¨åŠ 1
            return count
        except Exception as e:
            logger.error(f"Error on Getting Document File Count in Folder '{folder_name}'; {e}.")
            return 0
    
    def chat(self, message: str) -> str:
        """
        Chat with the system
        
        Args:
            message: User message
            
        Returns:
            System response
        """
        if self.chat_engine is None:
            raise ValueError("Please Create Global Chat Engine First.")
        
        try:
            response = self.chat_engine.chat(message)
            return str(response)
        except Exception as e:
            logger.error(f"Error During Chat: {e}.")
            return f"Sorry, Error Occurred When Processing the Question; {e}."
    
    def list_documents(self) -> List[str]:  # å®Œå…¨æ”¯æŒæœ‰å¤šå±‚å­æ–‡ä»¶å¤¹çš„ç›®å½•ç»“æ„
        """
        List all documents in system (recursively scan subdirectories)
        
        Returns:
            List of document filenames (including relative paths)
        """
        try:
            files = []
            # Recursively scan files in all subdirectories
            for file_path in self.documents_dir.rglob("*"):  # ä¼šé€’å½’éå†documents_dirä¸‹çš„æ‰€æœ‰å†…å®¹ï¼ŒåŒ…æ‹¬æ‰€æœ‰å­æ–‡ä»¶å¤¹é‡Œçš„æ–‡ä»¶ï¼Œä¸ç®¡æœ‰å‡ å±‚ã€‚
                if file_path.is_file():
                    # Get path relative to documents directory
                    relative_path = file_path.relative_to(self.documents_dir)  # ä¼šæŠŠæ¯ä¸ªæ–‡ä»¶çš„å…¨è·¯å¾„è½¬æ¢æˆç›¸å¯¹äºæ ¹ç›®å½•çš„è·¯å¾„ï¼Œæ¯”å¦‚ï¼š
                    files.append(str(relative_path))
            return sorted(files)  # æœ€ç»ˆæ‰€æœ‰æ–‡ä»¶ï¼ˆæ— è®ºåœ¨å“ªä¸€å±‚ï¼‰éƒ½ä¼šè¢«åŒ…å«è¿›è¿”å›ç»“æœï¼Œè€Œä¸”æ˜¯æ¸…æ¥šçš„â€œå¸¦å±‚æ¬¡â€çš„ç›¸å¯¹è·¯å¾„ã€‚
        except Exception as e:
            logger.error(f"Error on Listing Document: {e}.")
            return []
        