{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c850f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "DEEPSEEK_MODELS = [\"deepseek-chat\"]\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35cc7999-129b-40cc-9a1e-ebdd4287150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"4o-QA.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc6f6f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMEvaluator:\n",
    "    def __init__(self, deepseek_api_key: str = None):\n",
    "        self.deepseek_api_key = deepseek_api_key\n",
    "        self.client = OpenAI(\n",
    "            api_key=self.deepseek_api_key,\n",
    "            base_url=\"https://api.deepseek.com\"\n",
    "        )\n",
    "    \n",
    "    def call_deepseek_api(self, prompt: str, model: str = \"deepseek-chat\") -> str:\n",
    "        if model == \"deepseek-chat\":\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1,\n",
    "                max_tokens=2000,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            # print(response.choices[0].message.content.strip())\n",
    "            return response.choices[0].message.content.strip()\n",
    "\n",
    "    \n",
    "    def save_to_json(self, results, output_path):\n",
    "        with open(output_path, 'w', encoding='utf-8') as jsonfile:\n",
    "            json.dump(results, jsonfile, indent=2, ensure_ascii=False)\n",
    "        print(f\"Evaluation Results Saved in JSON: {output_path}\")\n",
    "\n",
    "    \n",
    "    def evaluate_with_llm(self, question, candidate, model=\"deepseek-chat\"):\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Below is a question and a student's answer based on literature about Additive Manufacturing, specifically regarding Fused Deposition Modeling (FDM) and Polylactic Acid (PLA). \n",
    "The question and the referenced literature may also include content related to PLA composites.\n",
    "\n",
    "**Question:**\n",
    "{question}\n",
    "\n",
    "**Student Answer:**\n",
    "{candidate}\n",
    "\n",
    "Evaluate the answer on the following dimensions (score each from 1 to 10, decimals allowed, 10 = best):\n",
    "\n",
    "1. Answer_Relevance: The degree to which the content of the student's answer is relevant to the question.\n",
    "2. Context_Precision: The proportion of information in the student's answer that is relevant to the question, relative to all information provided in the answer.\n",
    "3. Completeness: Whether the student's answer is complete, for example, whether there are any incomplete sentences.\n",
    "4. Coherency: Whether the meaning of the student's answer is smooth and clear, for example, how well the sentences are connected to each other.\n",
    "5. Formatting_Quality: Evaluate the use of Markdown. Consider whether Markdown syntax is present and whether the formatting meaningfully improves readability and organization.\n",
    "\n",
    "Also provide an Explanation: Give the rationale for each score or identify potential issues, covering all dimensions.\n",
    "\n",
    "Output only valid JSON in the following format, with no additional text outside the JSON. \n",
    "The values in this example are placeholders and should not be taken as the reference.\n",
    "\n",
    "{{\n",
    "  \"Answer_Relevance\": 5.0,\n",
    "  \"Context_Precision\": 5.0,\n",
    "  \"Completeness\": 5.0,\n",
    "  \"Coherency\": 5.0,\n",
    "  \"Formatting_Quality\": 5.0,\n",
    "  \"Explanation\": \"Scoring rationale or potential issues if exist.\"\n",
    "}}\n",
    "\n",
    "Note:\n",
    "- Evaluate objectively and rigorously, and do not be lenient in scoring.\n",
    "- Score EACH dimension independently. Do not blend dimensions or compensate a low score in one dimension with a high score in another. Do not compute or imply an overall/average score.\n",
    "- Completeness: if any sentence or paragraph is clearly truncated, apply an additional penalty on this dimension, as truncation indicates missing content.\n",
    "- Formatting_Quality: if the answer, even when short, could be made clearer with basic Markdown but none is used, deduct points; also deduct for incorrect or harmful formatting.\n",
    "\"\"\"\n",
    "\n",
    "        response = self.call_deepseek_api(prompt, model)\n",
    "        \n",
    "        if model == \"deepseek-chat\":\n",
    "            start_idx = response.find('{')\n",
    "            end_idx = response.rfind('}') + 1\n",
    "            json_str = response[start_idx:end_idx]\n",
    "            result = json.loads(json_str)\n",
    "            \n",
    "            # regulate the score format, without influencing the \"evaluation\" dimension\n",
    "            for key in [\"Answer_Relevance\", \"Context_Precision\", \"Completeness\", \"Coherency\", \"Formatting_Quality\"]:\n",
    "                if key in result:\n",
    "                    result[key] = max(1.0, min(10.0, float(result[key])))\n",
    "                else:\n",
    "                    result[key] = 0\n",
    "            return result            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61d51bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = LLMEvaluator(deepseek_api_key=DEEPSEEK_API_KEY)\n",
    "df = pd.read_excel(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ab4f2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Evaluation of 45 Data Rows.\n",
      "==================================================\n",
      "Evaluating Using deepseek-chat.\n",
      "   Processing Row 1/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 2/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 3/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 4/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 5/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 6/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 7/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 8/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 9/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 10/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 11/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 12/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 13/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 14/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 15/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 16/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 17/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 18/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 19/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 20/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 21/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 22/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 23/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 24/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 25/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 26/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 27/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 28/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 29/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 30/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 31/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 32/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 33/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 34/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 35/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 36/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 37/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 38/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 39/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 40/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 41/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 42/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 43/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 44/45... LLM Evaluation for This Row Completed.\n",
      "   Processing Row 45/45... LLM Evaluation for This Row Completed.\n",
      "Evaluation Using deepseek-chat Completed.\n"
     ]
    }
   ],
   "source": [
    "def evaluate_responses_with_models(df):\n",
    "    model_results = {}\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    print(f\"Start Evaluation of {total_rows} Data Rows.\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for model_idx, model in enumerate(DEEPSEEK_MODELS, 1):\n",
    "        print(f\"Evaluating Using {model}.\")\n",
    "        results = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            question = str(row.iloc[0])\n",
    "            response = str(row.iloc[1])\n",
    "            print(f\"   Processing Row {idx + 1}/{total_rows}...\", end=\" \")\n",
    "            \n",
    "            evaluation_results = evaluator.evaluate_with_llm(\n",
    "                question=question,\n",
    "                candidate=response,\n",
    "                model=model\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'question': question,\n",
    "                'response': response,\n",
    "                'LLM_evaluation_results': evaluation_results\n",
    "            })\n",
    "            print(\"LLM Evaluation for This Row Completed.\")\n",
    "\n",
    "        \n",
    "        dimensions = [\"Answer_Relevance\", \"Context_Precision\", \"Completeness\", \"Coherency\", \"Formatting_Quality\"]\n",
    "        averages = {}\n",
    "\n",
    "        # calculate the average score\n",
    "        for dim in dimensions:\n",
    "            scores = [r[\"LLM_evaluation_results\"].get(dim, 0) for r in results]\n",
    "            averages[dim] = np.mean(scores)\n",
    "        \n",
    "        model_results[model] = {\n",
    "            'individual_results': results,\n",
    "            'averages': averages\n",
    "        }\n",
    "        \n",
    "        print(f\"Evaluation Using {model} Completed.\")\n",
    "    return model_results\n",
    "\n",
    "results = evaluate_responses_with_models(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878c58c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEEPSEEK-CHAT Evaluation Results:\n",
      "  Answer_Relevance: 9.40/10\n",
      "  Context_Precision: 8.99/10\n",
      "  Completeness: 9.64/10\n",
      "  Coherency: 9.38/10\n",
      "  Formatting_Quality: 6.91/10\n",
      "  Overall Average: 8.86/10\n"
     ]
    }
   ],
   "source": [
    "for model, model_data in results.items():\n",
    "    print(f\"\\n{model.upper()} Evaluation Results:\")\n",
    "    avg = model_data['averages']\n",
    "    print(f\"  Answer_Relevance: {avg['Answer_Relevance']:.2f}/10\")\n",
    "    print(f\"  Context_Precision: {avg['Context_Precision']:.2f}/10\")\n",
    "    print(f\"  Completeness: {avg['Completeness']:.2f}/10\")\n",
    "    print(f\"  Coherency: {avg['Coherency']:.2f}/10\")\n",
    "    print(f\"  Formatting_Quality: {avg['Formatting_Quality']:.2f}/10\")\n",
    "    \n",
    "    overall_avg = np.mean([avg['Answer_Relevance'], avg['Context_Precision'], avg['Completeness'], avg['Coherency'], avg['Formatting_Quality']])\n",
    "    print(f\"  Overall Average: {overall_avg:.2f}/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87b06b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results Saved in JSON: 4o-QA-3.json\n"
     ]
    }
   ],
   "source": [
    "evaluator.save_to_json(results, \"4o-QA-3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e36683-d980-4bfa-b71e-56fd0dc7e671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
